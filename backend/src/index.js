<<<<<<< HEAD
(()=>{"use strict";var e={490:function(e,t,a){var s=this&&this.__createBinding||(Object.create?function(e,t,a,s){void 0===s&&(s=a);var n=Object.getOwnPropertyDescriptor(t,a);n&&!("get"in n?!t.__esModule:n.writable||n.configurable)||(n={enumerable:!0,get:function(){return t[a]}}),Object.defineProperty(e,s,n)}:function(e,t,a,s){void 0===s&&(s=a),e[s]=t[a]}),n=this&&this.__setModuleDefault||(Object.create?function(e,t){Object.defineProperty(e,"default",{enumerable:!0,value:t})}:function(e,t){e.default=t}),r=this&&this.__importStar||function(e){if(e&&e.__esModule)return e;var t={};if(null!=e)for(var a in e)"default"!==a&&Object.prototype.hasOwnProperty.call(e,a)&&s(t,e,a);return n(t,e),t},o=this&&this.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0}),t.handleGetTracks=t.handleGetPackageCost=t.handleGetPackageRating=t.handleSearchPackagesByRegEx=t.handleGetPackageHistoryByName=t.handleResetRegistry=t.handleListPackages=t.handleDeletePackage=t.handleUpdatePackage=t.handleRetrievePackage=t.handleCreatePackage=t.handleAuthenticate=void 0;const c=r(a(59)),i=a(139),d=a(748),u=a(521),p=o(a(829)),g=o(a(486));t.handleAuthenticate=async e=>{const{User:t,Secret:a}=JSON.parse(e),{name:s,isAdmin:n}=t,{password:r}=a;if(!s||"boolean"!=typeof n||!r)return(0,d.sendResponse)(400,{message:"Missing fields in AuthenticationRequest"});try{const e=await(0,c.getUserByName)(s);if(!e)return(0,d.sendResponse)(401,{message:"Invalid user or password."});if(!await g.default.compare(r,e.password_hash))return(0,d.sendResponse)(401,{message:"Invalid user or password."});const t=p.default.sign({sub:e.id,name:e.name,isAdmin:e.isAdmin},process.env.JWT_SECRET,{expiresIn:process.env.JWT_EXPIRES_IN||"1h"});return(0,d.sendResponse)(200,{token:`bearer ${t}`})}catch(e){return console.error("Authentication Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleCreatePackage=async(e,t)=>{const a=JSON.parse(e),{metadata:s,data:n}=a;if(!(s&&s.Name&&s.Version&&s.ID))return(0,d.sendResponse)(400,{message:"Missing required package metadata fields."});if(n.Content&&n.URL||!n.Content&&!n.URL)return(0,d.sendResponse)(400,{message:"Either Content or URL must be set, but not both."});try{const e=await(0,c.createPackage)(s,n);n.Content&&await(0,i.uploadPackageContent)(s.ID,n.Content);const t="\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    ";return await c.default.query(t,[s.ID,5445,"CREATE"]),(0,d.sendResponse)(201,e)}catch(e){return console.error("Create Package Error:",e),"23505"===e.code?(0,d.sendResponse)(409,{message:"Package exists already."}):(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleRetrievePackage=async(e,t)=>{let a;try{a=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{const t="SELECT * FROM packages WHERE id = $1",s=await c.default.query(t,[e]);if(0===s.rows.length)return(0,d.sendResponse)(404,{message:"Package does not exist."});const n=s.rows[0];if(console.log(s.rows[0]),!n.data.Content&&!n.data.URL)try{const t=await(0,i.getPackageContent)(e);n.data.Content=t}catch(e){return console.error("S3 Retrieval Error:",e),(0,d.sendResponse)(500,{message:"Failed to retrieve package content."})}const r="\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    ";return await c.default.query(r,[e,a.sub,"DOWNLOAD"]),(0,d.sendResponse)(200,n)}catch(e){return console.error("Retrieve Package Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleUpdatePackage=async(e,t,a)=>{let s;try{s=(0,u.authenticate)(a)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}const n=JSON.parse(t),{metadata:r,data:o}=n;if(r.ID!==e)return(0,d.sendResponse)(400,{message:"Metadata ID does not match the path ID."});try{const t=[],a=[];let n=1;o.Content?(t.push("content = $"+n++),a.push(o.Content)):o.URL&&(t.push("url = $"+n++),a.push(o.URL)),t.push("debloat = $"+n++),a.push(o.debloat||!1),o.JSProgram&&(t.push("js_program = $"+n++),a.push(o.JSProgram)),t.push("updated_at = NOW()");const r=`UPDATE packages SET ${t.join(", ")} WHERE id = $${n} RETURNING *`;a.push(e);const u=await c.default.query(r,a);if(0===u.rows.length)return(0,d.sendResponse)(404,{message:"Package does not exist."});const p=u.rows[0];o.Content&&await(0,i.uploadPackageContent)(e,o.Content);const g="\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    ";return await c.default.query(g,[e,s.sub,"UPDATE"]),(0,d.sendResponse)(200,p)}catch(e){return console.error("Update Package Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleDeletePackage=async(e,t)=>{let a;try{a=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{const t="DELETE FROM packages WHERE id = $1 RETURNING *",s=await c.default.query(t,[e]);if(0===s.rows.length)return(0,d.sendResponse)(404,{message:"Package does not exist."});s.rows[0].data.Content&&await(0,i.deletePackageContent)(e);const n="\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    ";return await c.default.query(n,[e,a.sub,"DELETE"]),(0,d.sendResponse)(200,{message:"Package is deleted."})}catch(e){return console.error("Delete Package Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleListPackages=async(e,t,a)=>{const s=JSON.parse(e),n=a&&a.offset?parseInt(a.offset):0;if(!Array.isArray(s))return(0,d.sendResponse)(400,{message:"Request body must be an array of PackageQuery."});try{const e=[];for(const t of s){const{Name:a,Version:s}=t;if(!a)return(0,d.sendResponse)(400,{message:"PackageQuery must include Name."});let n="SELECT * FROM packages WHERE name ILIKE $1";const r=[`%${a}%`];s&&(n+=" AND version = $2",r.push(s));const o=await c.default.query(n,r);e.push(...o.rows)}const t=e.slice(n,n+10),a=n+10<e.length?n+10:null,r={};return null!==a&&(r.offset=a.toString()),{statusCode:200,headers:r,body:JSON.stringify(t)}}catch(e){return console.error("List Packages Error:",e),e.message.includes("too many")?(0,d.sendResponse)(413,{message:"Too many packages returned."}):(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleResetRegistry=async e=>{let t;try{if(t=(0,u.authenticate)(e),!t.isAdmin)return(0,d.sendResponse)(403,{message:"Admin privileges required."})}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{return await c.default.query("TRUNCATE TABLE packages CASCADE;"),await c.default.query("TRUNCATE TABLE package_history CASCADE;"),(0,d.sendResponse)(200,{message:"Registry is reset."})}catch(e){return console.error("Reset Registry Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleGetPackageHistoryByName=async(e,t)=>{let a;try{a=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{const t="\n      SELECT ph.*, u.name as user_name, u.is_admin\n      FROM package_history ph\n      JOIN packages p ON ph.package_id = p.id\n      JOIN users u ON ph.user_id = u.id\n      WHERE p.name ILIKE $1\n      ORDER BY ph.date DESC\n    ",a=await c.default.query(t,[`%${e}%`]);if(0===a.rows.length)return(0,d.sendResponse)(404,{message:"No such package."});const s=a.rows.map((e=>({User:{name:e.user_name,isAdmin:e.is_admin},Date:e.date.toISOString(),PackageMetadata:{Name:e.name,Version:e.version,ID:e.package_id},Action:e.action})));return(0,d.sendResponse)(200,s)}catch(e){return console.error("Get Package History Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleSearchPackagesByRegEx=async(e,t)=>{let a;try{a=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}const{RegEx:s}=JSON.parse(e);if(!s)return(0,d.sendResponse)(400,{message:"Missing RegEx field in PackageRegEx."});try{const e="\n      SELECT * FROM packages\n      WHERE name ~* $1 OR readme ~* $1\n    ",t=await c.default.query(e,[s]);if(0===t.rows.length)return(0,d.sendResponse)(404,{message:"No package found under this regex."});const a=t.rows.map((e=>({Version:e.version,Name:e.name,ID:e.id})));return(0,d.sendResponse)(200,a)}catch(e){return console.error("Search Packages Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleGetPackageRating=async(e,t)=>{let a;try{a=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{const t="SELECT * FROM package_ratings WHERE package_id = $1",a=await c.default.query(t,[e]);if(0===a.rows.length)return(0,d.sendResponse)(500,{message:"The package rating system choked on at least one of the metrics."});const s=a.rows[0];return(0,d.sendResponse)(200,s)}catch(e){return console.error("Get Package Rating Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleGetPackageCost=async(e,t,a)=>{let s;try{s=(0,u.authenticate)(t)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}const n=a&&"true"===a.dependency;try{const t="\n      SELECT p.id, p.name, p.version, p.size_mb\n      FROM packages p\n      WHERE p.id = $1\n    ";if(0===(await c.default.query(t,[e])).rows.length)return(0,d.sendResponse)(404,{message:"Package does not exist."});const a={};let s=0;const r=[e],o=new Set;for(;r.length>0;){const e=r.pop();if(o.has(e))continue;o.add(e);const t="SELECT * FROM packages WHERE id = $1",i=await c.default.query(t,[e]);if(0===i.rows.length)continue;const d=i.rows[0];if(a[d.id]={standaloneCost:d.size_mb,totalCost:d.size_mb},s+=d.size_mb,n){const t="SELECT dependency_id FROM dependencies WHERE package_id = $1";(await c.default.query(t,[e])).rows.forEach((e=>{o.has(e.dependency_id)||(r.push(e.dependency_id),s+=e.size_mb)}))}}if(n){let e=0;for(const t in a)e+=a[t].standaloneCost||0,a[t].totalCost=e}else a[e].totalCost=a[e].standaloneCost||0;return(0,d.sendResponse)(200,a)}catch(e){return console.error("Get Package Cost Error:",e),(0,d.sendResponse)(500,{message:"Internal server error."})}},t.handleGetTracks=async e=>{let t;try{t=(0,u.authenticate)(e)}catch(e){return(0,d.sendResponse)(e.statusCode,{message:e.message})}try{const e="\n      SELECT t.track_name\n      FROM user_tracks ut\n      JOIN tracks t ON ut.track_id = t.id\n      WHERE ut.user_id = $1\n    ",a=(await c.default.query(e,[t.sub])).rows.map((e=>e.track_name));return(0,d.sendResponse)(200,{plannedTracks:a})}catch(e){return console.error("Get Tracks Error:",e),(0,d.sendResponse)(500,{message:"The system encountered an error while retrieving the student's track information."})}}},59:function(e,t,a){var s=this&&this.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0}),t.getPackagedatabyID=t.createPackage=t.getUserByName=void 0;const n=s(a(818)),r=a(449);n.default.config();const o=new r.Pool({host:process.env.RDS_HOST,user:process.env.RDS_USER,password:process.env.RDS_PASSWORD,database:process.env.RDS_DATABASE,port:parseInt(process.env.RDS_PORT||"5432"),max:20,idleTimeoutMillis:3e4,connectionTimeoutMillis:2e3,ssl:{rejectUnauthorized:!1}});t.default=o,t.getUserByName=async e=>{const t=await o.query("SELECT * FROM users WHERE name = $1",[e]);return 0===t.rows.length?null:t.rows[0]},t.createPackage=async(e,t)=>{const a=[e.ID,e.Name,e.Version,t.Content||null,t.URL||null,t.debloat||!1,t.JSProgram||null];return(await o.query("\n    INSERT INTO packages (id, name, version, content, url, debloat, js_program)\n    VALUES ($1, $2, $3, $4, $5, $6, $7)\n    RETURNING *;\n  ",a)).rows[0]},t.getPackagedatabyID=async e=>{const t=await o.query("select id,name,version,content from packages as p where p.id=$1",[e]);return 0===t.rows.length?null:t.rows[0]}},139:function(e,t,a){var s=this&&this.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0}),t.deletePackageContent=t.getPackageContent=t.uploadPackageContent=void 0;const n=new(s(a(496)).default.S3);t.uploadPackageContent=async(e,t)=>{const a={Bucket:process.env.S3_BUCKET,Key:`packages/${e}.zip`,Body:Buffer.from(t,"base64")};await n.putObject(a).promise()},t.getPackageContent=async e=>{const t={Bucket:process.env.S3_BUCKET,Key:`packages/${e}.zip`};return(await n.getObject(t).promise()).Body.toString("base64")},t.deletePackageContent=async e=>{const t={Bucket:process.env.S3_BUCKET,Key:`packages/${e}.zip`};await n.deleteObject(t).promise()}},521:function(e,t,a){var s=this&&this.__importDefault||function(e){return e&&e.__esModule?e:{default:e}};Object.defineProperty(t,"__esModule",{value:!0}),t.authenticate=void 0;const n=s(a(829));t.authenticate=e=>{const t=e["X-Authorization"]||e["x-authorization"];if(!t)throw{statusCode:403,message:"Missing Authentication Token"};const a=t.split(" ");if(2!==a.length||"bearer"!==a[0].toLowerCase())throw{statusCode:403,message:"Invalid Authentication Token"};const s=a[1];try{return n.default.verify(s,process.env.JWT_SECRET)}catch(e){throw{statusCode:403,message:"Invalid Authentication Token"}}}},748:(e,t)=>{Object.defineProperty(t,"__esModule",{value:!0}),t.sendResponse=void 0,t.sendResponse=(e,t)=>({statusCode:e,headers:{"Content-Type":"application/json"},body:JSON.stringify(t)})},496:e=>{e.exports=require("aws-sdk")},486:e=>{e.exports=require("bcrypt")},818:e=>{e.exports=require("dotenv")},829:e=>{e.exports=require("jsonwebtoken")},449:e=>{e.exports=require("pg")}},t={};function a(s){var n=t[s];if(void 0!==n)return n.exports;var r=t[s]={exports:{}};return e[s].call(r.exports,r,r.exports,a),r.exports}var s={};(()=>{var e=s;Object.defineProperty(e,"__esModule",{value:!0}),e.handler=void 0;const t=a(490);e.handler=async e=>{const{httpMethod:a,path:s,pathParameters:n,queryStringParameters:r,headers:o,body:c}=e;console.log(`Received event: ${JSON.stringify(e)}`);try{if("/authenticate"===s&&"PUT"===a)return await(0,t.handleAuthenticate)(c||"{}");if("/packages"===s&&"POST"===a)return await(0,t.handleListPackages)(c||"[]",o,r||{});if("/reset"===s&&"DELETE"===a)return await(0,t.handleResetRegistry)(o);if("/package/byRegEx"===s&&"POST"===a)return await(0,t.handleSearchPackagesByRegEx)(c||"{}",o);if(s&&s.startsWith("/package/byName/")&&"GET"===a){const e=s.split("/").pop()||"";return await(0,t.handleGetPackageHistoryByName)(e,o)}if("/package"===s&&"POST"===a)return await(0,t.handleCreatePackage)(c||"{}",o);if(s&&s.startsWith("/package/")&&s.endsWith("/rate")&&"GET"===a){const e=s.split("/")[2];return await(0,t.handleGetPackageRating)(e,o)}if(s&&s.startsWith("/package/")&&s.endsWith("/cost")&&"GET"===a){const e=s.split("/")[2];return await(0,t.handleGetPackageCost)(e,o,r||{})}if(s&&s.startsWith("/package/")&&"GET"===a){const e=s.split("/")[2];return await(0,t.handleRetrievePackage)(e,o)}if(s&&s.startsWith("/package/")&&"PUT"===a){const e=s.split("/")[2];return await(0,t.handleUpdatePackage)(e,c||"{}",o)}if(s&&s.startsWith("/package/")&&"DELETE"===a){const e=s.split("/")[2];return await(0,t.handleDeletePackage)(e,o)}return"/tracks"===s&&"GET"===a?await(0,t.handleGetTracks)(o):{statusCode:404,body:JSON.stringify({message:"Endpoint not found."}),headers:{"Content-Type":"application/json"}}}catch(e){return console.error("Lambda Handler Error:",e),e.statusCode&&e.message?{statusCode:e.statusCode,body:JSON.stringify({message:e.message}),headers:{"Content-Type":"application/json"}}:{statusCode:500,body:JSON.stringify({message:"Internal server error."}),headers:{"Content-Type":"application/json"}}}}})(),module.exports=s})();
=======
/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ (() => { // webpackBootstrap
/******/ 	var __webpack_modules__ = ({

/***/ "./handlerhelper.ts":
/*!**************************!*\
  !*** ./handlerhelper.ts ***!
  \**************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.generatePackageId = exports.metricCalcFromUrlUsingNetScore = void 0;\nconst processURL_1 = __webpack_require__(/*! ./rating/processURL */ \"./rating/processURL.ts\");\nconst logger_1 = __webpack_require__(/*! ./rating/logger */ \"./rating/logger.ts\");\nconst netScore_1 = __webpack_require__(/*! ./rating/metrics/netScore */ \"./rating/metrics/netScore.ts\");\nconst crypto_1 = __importDefault(__webpack_require__(/*! crypto */ \"crypto\"));\nconst logger = (0, logger_1.getLogger)();\nasync function metricCalcFromUrlUsingNetScore(url) {\n    const repoInfo = await (0, processURL_1.getGithubRepoInfoFromUrl)(url);\n    logger.info(\"repoInfo:\", repoInfo);\n    if (repoInfo == null) {\n        return null;\n    }\n    // Calculate Net Score\n    const netScoreJSON = await (0, netScore_1.calculateNetScore)(undefined, repoInfo);\n    if (!netScoreJSON) {\n        return null;\n    }\n    const NetScore = netScoreJSON.NetScore;\n    const RampUp = netScoreJSON.RampUp;\n    const Correctness = netScoreJSON.Correctness;\n    const BusFactor = netScoreJSON.BusFactor;\n    const ResponsiveMaintainer = netScoreJSON.ResponsiveMaintainer;\n    const License = netScoreJSON.License;\n    // Placeholder values for Pull Requests Score and Pinned Dependencies Score\n    const pullRequestsScore = 1; // Placeholder value\n    const pinnedDependenciesScore = 1; // Placeholder value\n    const currentRepoInfoScores = {\n        ID: \"\",\n        NAME: repoInfo.repo,\n        OWNER: repoInfo.owner,\n        VERSION: \"1.0.0\",\n        URL: repoInfo.url,\n        NET_SCORE: NetScore,\n        RAMP_UP_SCORE: RampUp,\n        CORRECTNESS_SCORE: Correctness,\n        BUS_FACTOR_SCORE: BusFactor,\n        RESPONSIVE_MAINTAINER_SCORE: ResponsiveMaintainer,\n        LICENSE_SCORE: License,\n        PULL_REQUESTS_SCORE: pullRequestsScore,\n        PINNED_DEPENDENCIES_SCORE: pinnedDependenciesScore,\n    };\n    return currentRepoInfoScores;\n}\nexports.metricCalcFromUrlUsingNetScore = metricCalcFromUrlUsingNetScore;\nconst generatePackageId = (name, version) => {\n    console.log(`Generating id for ${name}@${version}`);\n    // Generate a random 63-bit integer to ensure it fits within the positive range of BIGINT\n    const randomBytes = crypto_1.default.randomBytes(8); // 8 bytes = 64 bits\n    let id = BigInt(`0x${randomBytes.toString('hex')}`);\n    // Ensure the ID is within the positive range of BIGINT\n    const maxBigInt = BigInt('9223372');\n    id = id % maxBigInt;\n    console.log(`Generated package id ${id.toString()} for ${name}@${version}`);\n    return id.toString();\n};\nexports.generatePackageId = generatePackageId;\n\n\n//# sourceURL=webpack://lambda-api-handler/./handlerhelper.ts?");

/***/ }),

/***/ "./handlers/handlers.ts":
/*!******************************!*\
  !*** ./handlers/handlers.ts ***!
  \******************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\n// src/handlers/handlers.ts\nvar __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    var desc = Object.getOwnPropertyDescriptor(m, k);\n    if (!desc || (\"get\" in desc ? !m.__esModule : desc.writable || desc.configurable)) {\n      desc = { enumerable: true, get: function() { return m[k]; } };\n    }\n    Object.defineProperty(o, k2, desc);\n}) : (function(o, m, k, k2) {\n    if (k2 === undefined) k2 = k;\n    o[k2] = m[k];\n}));\nvar __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {\n    Object.defineProperty(o, \"default\", { enumerable: true, value: v });\n}) : function(o, v) {\n    o[\"default\"] = v;\n});\nvar __importStar = (this && this.__importStar) || function (mod) {\n    if (mod && mod.__esModule) return mod;\n    var result = {};\n    if (mod != null) for (var k in mod) if (k !== \"default\" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);\n    __setModuleDefault(result, mod);\n    return result;\n};\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.handleGetTracks = exports.handleGetPackageCost = exports.handleGetPackageRating = exports.handleSearchPackagesByRegEx = exports.handleGetPackageHistoryByName = exports.handleResetRegistry = exports.handleListPackages = exports.handleDeletePackage = exports.handleUpdatePackage = exports.handleRetrievePackage = exports.handleCreatePackage = exports.handleAuthenticate = void 0;\nconst dbService_1 = __importStar(__webpack_require__(/*! ../services/dbService */ \"./services/dbService.ts\"));\nconst s3Service_1 = __webpack_require__(/*! ../services/s3Service */ \"./services/s3Service.ts\");\nconst response_1 = __webpack_require__(/*! ../utils/response */ \"./utils/response.ts\");\nconst auth_1 = __webpack_require__(/*! ../utils/auth */ \"./utils/auth.ts\");\nconst handlerhelper_1 = __webpack_require__(/*! ../handlerhelper */ \"./handlerhelper.ts\");\nconst logger_1 = __webpack_require__(/*! ../rating/logger */ \"./rating/logger.ts\");\nconst adm_zip_1 = __importDefault(__webpack_require__(/*! adm-zip */ \"../../node_modules/adm-zip/adm-zip.js\"));\nconst jsonwebtoken_1 = __importDefault(__webpack_require__(/*! jsonwebtoken */ \"jsonwebtoken\"));\nconst bcrypt_1 = __importDefault(__webpack_require__(/*! bcrypt */ \"bcrypt\"));\nconst logger = (0, logger_1.getLogger)();\n// Handler for /authenticate - PUT\nconst handleAuthenticate = async (body) => {\n    const { User, Secret } = JSON.parse(body);\n    const { name, isAdmin } = User;\n    const { password } = Secret;\n    if (!name || typeof isAdmin !== 'boolean' || !password) {\n        return (0, response_1.sendResponse)(400, { message: 'Missing fields in AuthenticationRequest' });\n    }\n    try {\n        // Fetch user from DB\n        const user = await (0, dbService_1.getUserByName)(name);\n        if (!user) {\n            return (0, response_1.sendResponse)(401, { message: 'Invalid user or password.' });\n        }\n        // Compare password\n        const isMatch = await bcrypt_1.default.compare(password, user.password_hash);\n        if (!isMatch) {\n            return (0, response_1.sendResponse)(401, { message: 'Invalid user or password.' });\n        }\n        // Generate JWT\n        const token = jsonwebtoken_1.default.sign({ sub: user.id, name: user.name, isAdmin: user.isAdmin }, process.env.JWT_SECRET, { expiresIn: process.env.JWT_EXPIRES_IN || '1h' });\n        return (0, response_1.sendResponse)(200, { token: `bearer ${token}` });\n    }\n    catch (error) {\n        console.error('Authentication Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleAuthenticate = handleAuthenticate;\n// Handler for /package - POST (Create Package)\nconst handleCreatePackage = async (body, headers) => {\n    var _a, _b, _c;\n    // Authenticate the request\n    const fetch = (await Promise.resolve(/*! import() */).then(__webpack_require__.t.bind(__webpack_require__, /*! node-fetch */ \"node-fetch\", 23))).default;\n    let user = {\n        sub: 5445,\n        name: \"admin\",\n        isAdmin: true,\n        iat: 545,\n        exp: 45\n    };\n    // try {\n    //   user = authenticate(headers);\n    // } catch (err: any) {\n    //   return sendResponse(err.statusCode, { message: err.message });\n    // }\n    const packageData = JSON.parse(body);\n    const { metadata, data } = packageData;\n    const pkgName = metadata.Name;\n    const pkgVersion = metadata.Version;\n    const pkgeId = (0, handlerhelper_1.generatePackageId)(pkgName, pkgVersion);\n    // Validate required fields\n    if (!metadata || !metadata.Name || !metadata.Version || !metadata.ID) {\n        return (0, response_1.sendResponse)(400, { message: 'Missing required package metadata fields.' });\n    }\n    // Validate PackageData union type\n    if ((data.Content && data.URL) || (!data.Content && !data.URL)) {\n        return (0, response_1.sendResponse)(400, { message: 'Either Content or URL must be set, but not both.' });\n    }\n    try {\n        let info;\n        if (data.Content) {\n            logger.info(\"createPackage request via zip upload\");\n            const base64Data = data.Content;\n            if (!base64Data) {\n                logger.debug(\"Invalid base64-encoded data\");\n                return (0, response_1.sendResponse)(400, { error: 'Invalid base64-encoded data' });\n            }\n            const base64Buffer = Buffer.from(atob(base64Data), 'binary');\n            // Extract package.json from zip file\n            const zip = new adm_zip_1.default(base64Buffer);\n            const zipEntries = zip.getEntries();\n            let packageJSON = null;\n            let extractedPackageJson;\n            zipEntries.forEach((entry) => {\n                const entryPathParts = entry.entryName.split('/');\n                console.log(entryPathParts);\n                if (entryPathParts.length === 2 && entryPathParts[1] === 'package.json') {\n                    packageJSON = entry.getData().toString('utf8');\n                }\n            });\n            if (packageJSON == null) {\n                console.log('Invalid package creation request: No package.json found in zip');\n                return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: No package.json found in zip' });\n            }\n            else {\n                extractedPackageJson = JSON.parse(packageJSON);\n                logger.console(`repo url: ${(_a = extractedPackageJson === null || extractedPackageJson === void 0 ? void 0 : extractedPackageJson.repository) === null || _a === void 0 ? void 0 : _a.url}, name: ${extractedPackageJson.name}, version: ${extractedPackageJson.version}`);\n                if (!((_b = extractedPackageJson === null || extractedPackageJson === void 0 ? void 0 : extractedPackageJson.repository) === null || _b === void 0 ? void 0 : _b.url) || !extractedPackageJson.name || !extractedPackageJson.version) {\n                    return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: package.json must contain repository url, package name, and version' });\n                }\n            }\n            // Get the URL from the info in package.json\n            const repoUrl = (_c = extractedPackageJson === null || extractedPackageJson === void 0 ? void 0 : extractedPackageJson.repository) === null || _c === void 0 ? void 0 : _c.url;\n            info = await (0, handlerhelper_1.metricCalcFromUrlUsingNetScore)(repoUrl);\n            console.log(\"info\", info);\n            if (!info) {\n                console.error(\"No package info returned from URL:\", repoUrl);\n                return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: Could not get package info from URL' });\n            }\n            info.ID = pkgeId;\n            info.NAME = extractedPackageJson.name;\n            info.VERSION = extractedPackageJson.version;\n            info.URL = repoUrl;\n            if (info.NET_SCORE < 0.5) {\n                logger.console('Invalid package creation request: Package cannot be uploaded due to disqualifying rating.');\n                return (0, response_1.sendResponse)(424, { error: 'Invalid package creation request: Package cannot be uploaded due to disqualifying rating.' });\n            }\n            // Upload package content to S3\n            // await uploadToS3(process.env.AWS_S3_BUCKET_NAME || \"\", \"packages/\" + pkgeId + \".zip\", base64Buffer);\n        }\n        else if (data.URL) {\n            //logger.console(`createPackage request via public ingest`);\n            logger.console(`createPackage request via public ingest:${data.URL}`);\n            info = await (0, handlerhelper_1.metricCalcFromUrlUsingNetScore)(data.URL);\n            if (!info) {\n                console.log(\"info\", info);\n                console.error(\"No package info returned from URL:\", data.URL);\n                return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: Could not get package info from URL' });\n            }\n            info.ID = pkgeId;\n            if (info.NET_SCORE < 0.5) {\n                logger.console('Invalid package creation request: Package cannot be uploaded due to disqualifying rating.');\n                return (0, response_1.sendResponse)(424, { error: 'Invalid package creation request: Package cannot be uploaded due to disqualifying rating.' });\n            }\n            // Download package content from GitHub using info\n            const response = await fetch(`https://api.github.com/repos/${info.OWNER}/${info.NAME}/zipball/HEAD`, {\n                headers: {\n                    Authorization: process.env.GITHUB_TOKEN || \"\",\n                    Accept: 'application/vnd.github.v3+json',\n                },\n            });\n            if (!response.ok) {\n                logger.console('Invalid package creation request: Could not get GitHub url for zip package download');\n                return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: Could not get GitHub url' });\n            }\n            const zipBuffer = Buffer.from(await response.arrayBuffer());\n            // Upload package content to S3\n            // await uploadToS3(process.env.AWS_S3_BUCKET_NAME || \"\", \"packages/\" + pkgeId + \".zip\", zipBuffer);\n        }\n        else {\n            logger.console('Invalid package creation request: Bad set of Content and URL');\n            return (0, response_1.sendResponse)(400, { error: 'Invalid package creation request: Bad set of Content and URL' });\n        }\n        // // Store package metadata in PostgreSQL\n        //   await insertIntoDatabase(pkgeId, pkgName, pkgVersion, info.URL, defaultUser.name);\n        //   // Respond with a success message\n        //   logger.console('Package created successfully');\n        //   res.status(200).json({ message: 'Package created successfully' });\n        // } catch (error) {\n        //   console.error('Error handling POST /package:', error);\n        //   res.status(500).json({ error: 'Internal Server Error' });\n        // }\n        // Insert package into PostgreSQL\n        const createdPackage = await (0, dbService_1.createPackage)(metadata, data);\n        // If Content is provided, upload to S3\n        if (data.Content) {\n            await (0, s3Service_1.uploadPackageContent)(metadata.ID, data.Content);\n        }\n        // Log the creation in package_history\n        const historyInsert = `\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    `;\n        await dbService_1.default.query(historyInsert, [metadata.ID, user.sub, 'CREATE']);\n        return (0, response_1.sendResponse)(201, createdPackage);\n    }\n    catch (error) {\n        console.error('Create Package Error:', error);\n        if (error.code === '23505') { // Unique violation\n            return (0, response_1.sendResponse)(409, { message: 'Package exists already.' });\n        }\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleCreatePackage = handleCreatePackage;\n// Handler for /package/{id} - GET (Retrieve Package)\nconst handleRetrievePackage = async (id, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        const queryText = 'SELECT * FROM packages WHERE id = $1';\n        const res = await dbService_1.default.query(queryText, [id]);\n        if (res.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'Package does not exist.' });\n        }\n        const packageData = res.rows[0];\n        // If Content is null and URL is null, retrieve from S3\n        if (!packageData.data.Content && !packageData.data.URL) {\n            try {\n                const content = await (0, s3Service_1.getPackageContent)(id);\n                packageData.data.Content = content;\n            }\n            catch (s3Error) {\n                console.error('S3 Retrieval Error:', s3Error);\n                return (0, response_1.sendResponse)(500, { message: 'Failed to retrieve package content.' });\n            }\n        }\n        // Log the retrieval in package_history\n        const historyInsert = `\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    `;\n        await dbService_1.default.query(historyInsert, [id, user.sub, 'DOWNLOAD']);\n        return (0, response_1.sendResponse)(200, packageData);\n    }\n    catch (error) {\n        console.error('Retrieve Package Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleRetrievePackage = handleRetrievePackage;\n// Handler for /package/{id} - PUT (Update Package)\nconst handleUpdatePackage = async (id, body, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    const updatedPackage = JSON.parse(body);\n    const { metadata, data } = updatedPackage;\n    // Ensure the name, version, and ID match\n    if (metadata.ID !== id) {\n        return (0, response_1.sendResponse)(400, { message: 'Metadata ID does not match the path ID.' });\n    }\n    try {\n        // Update package data in PostgreSQL\n        const updateFields = [];\n        const updateValues = [];\n        let idx = 1;\n        if (data.Content) {\n            updateFields.push(`content = $${idx++}`);\n            updateValues.push(data.Content);\n        }\n        else if (data.URL) {\n            updateFields.push(`url = $${idx++}`);\n            updateValues.push(data.URL);\n        }\n        updateFields.push(`debloat = $${idx++}`);\n        updateValues.push(data.debloat || false);\n        if (data.JSProgram) {\n            updateFields.push(`js_program = $${idx++}`);\n            updateValues.push(data.JSProgram);\n        }\n        updateFields.push(`updated_at = NOW()`);\n        const updateText = `UPDATE packages SET ${updateFields.join(', ')} WHERE id = $${idx} RETURNING *`;\n        updateValues.push(id);\n        const res = await dbService_1.default.query(updateText, updateValues);\n        if (res.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'Package does not exist.' });\n        }\n        const updatedPkg = res.rows[0];\n        // If Content is provided, upload to S3\n        if (data.Content) {\n            await (0, s3Service_1.uploadPackageContent)(id, data.Content);\n        }\n        // Log the update in package_history\n        const historyInsert = `\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    `;\n        await dbService_1.default.query(historyInsert, [id, user.sub, 'UPDATE']);\n        return (0, response_1.sendResponse)(200, updatedPkg);\n    }\n    catch (error) {\n        console.error('Update Package Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleUpdatePackage = handleUpdatePackage;\n// Handler for /package/{id} - DELETE (Delete Package)\nconst handleDeletePackage = async (id, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        const deleteText = 'DELETE FROM packages WHERE id = $1 RETURNING *';\n        const res = await dbService_1.default.query(deleteText, [id]);\n        if (res.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'Package does not exist.' });\n        }\n        const deletedPackage = res.rows[0];\n        // Delete from S3 if Content is present\n        if (deletedPackage.data.Content) {\n            await (0, s3Service_1.deletePackageContent)(id);\n        }\n        // Log the deletion in package_history\n        const historyInsert = `\n      INSERT INTO package_history (package_id, user_id, action)\n      VALUES ($1, $2, $3)\n    `;\n        await dbService_1.default.query(historyInsert, [id, user.sub, 'DELETE']);\n        return (0, response_1.sendResponse)(200, { message: 'Package is deleted.' });\n    }\n    catch (error) {\n        console.error('Delete Package Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleDeletePackage = handleDeletePackage;\n// Handler for /packages - GET (List Packages)\nconst handleListPackages = async (body, headers, queryStringParameters) => {\n    // Authenticate the request\n    let user = {\n        sub: 5445,\n        name: \"admin\",\n        isAdmin: true,\n        iat: 545,\n        exp: 45\n    };\n    // try {\n    //   user = authenticate(headers);\n    // } catch (err: any) {\n    //   return sendResponse(err.statusCode, { message: err.message });\n    // }\n    const queries = JSON.parse(body); // Array of PackageQuery\n    const offset = queryStringParameters && queryStringParameters.offset ? parseInt(queryStringParameters.offset) : 0;\n    const limit = 10; // Define your page size\n    if (!Array.isArray(queries)) {\n        return (0, response_1.sendResponse)(400, { message: 'Request body must be an array of PackageQuery.' });\n    }\n    try {\n        const results = [];\n        for (const query of queries) {\n            const { Name, Version } = query;\n            // Validate PackageQuery\n            if (!Name) {\n                return (0, response_1.sendResponse)(400, { message: 'PackageQuery must include Name.' });\n            }\n            // Build SQL query based on PackageQuery\n            let sql = 'SELECT * FROM packages WHERE name ILIKE $1';\n            const values = [`%${Name}%`];\n            if (Version) {\n                sql += ' AND version = $2';\n                values.push(Version);\n            }\n            const packageResult = await dbService_1.default.query(sql, values);\n            results.push(...packageResult.rows);\n        }\n        // Pagination\n        const paginatedResults = results.slice(offset, offset + limit);\n        const nextOffset = offset + limit < results.length ? offset + limit : null;\n        const responseHeaders = {};\n        if (nextOffset !== null) {\n            responseHeaders['offset'] = nextOffset.toString();\n        }\n        return {\n            statusCode: 200,\n            headers: responseHeaders,\n            body: JSON.stringify(paginatedResults),\n        };\n    }\n    catch (error) {\n        console.error('List Packages Error:', error);\n        if (error.message.includes('too many')) { // Example condition\n            return (0, response_1.sendResponse)(413, { message: 'Too many packages returned.' });\n        }\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleListPackages = handleListPackages;\n// Handler for /reset - DELETE\nconst handleResetRegistry = async (headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n        if (!user.isAdmin) {\n            return (0, response_1.sendResponse)(403, { message: 'Admin privileges required.' });\n        }\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        // Reset the registry to default state\n        await dbService_1.default.query('TRUNCATE TABLE packages CASCADE;');\n        await dbService_1.default.query('TRUNCATE TABLE package_history CASCADE;');\n        // Insert default data as needed\n        return (0, response_1.sendResponse)(200, { message: 'Registry is reset.' });\n    }\n    catch (error) {\n        console.error('Reset Registry Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleResetRegistry = handleResetRegistry;\n// Handler for /package/byName/{name} - GET (Package History by Name)\nconst handleGetPackageHistoryByName = async (name, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        const historyQuery = `\n      SELECT ph.*, u.name as user_name, u.is_admin\n      FROM package_history ph\n      JOIN packages p ON ph.package_id = p.id\n      JOIN users u ON ph.user_id = u.id\n      WHERE p.name ILIKE $1\n      ORDER BY ph.date DESC\n    `;\n        const historyResult = await dbService_1.default.query(historyQuery, [`%${name}%`]);\n        if (historyResult.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'No such package.' });\n        }\n        // Format the response as per PackageHistoryEntry\n        const historyEntries = historyResult.rows.map(entry => ({\n            User: {\n                name: entry.user_name,\n                isAdmin: entry.is_admin,\n            },\n            Date: entry.date.toISOString(),\n            PackageMetadata: {\n                Name: entry.name,\n                Version: entry.version,\n                ID: entry.package_id,\n            },\n            Action: entry.action,\n        }));\n        return (0, response_1.sendResponse)(200, historyEntries);\n    }\n    catch (error) {\n        console.error('Get Package History Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleGetPackageHistoryByName = handleGetPackageHistoryByName;\n// Handler for /package/byRegEx - POST (Search Packages by RegEx)\nconst handleSearchPackagesByRegEx = async (body, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    const { RegEx } = JSON.parse(body);\n    if (!RegEx) {\n        return (0, response_1.sendResponse)(400, { message: 'Missing RegEx field in PackageRegEx.' });\n    }\n    try {\n        // Use PostgreSQL regex matching on name and README (assuming README is a field)\n        const searchQuery = `\n      SELECT * FROM packages\n      WHERE name ~* $1 OR readme ~* $1\n    `;\n        const res = await dbService_1.default.query(searchQuery, [RegEx]);\n        if (res.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'No package found under this regex.' });\n        }\n        const packages = res.rows.map(pkg => ({\n            Version: pkg.version,\n            Name: pkg.name,\n            ID: pkg.id,\n        }));\n        return (0, response_1.sendResponse)(200, packages);\n    }\n    catch (error) {\n        console.error('Search Packages Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleSearchPackagesByRegEx = handleSearchPackagesByRegEx;\n// Handler for /package/{id}/rate - GET (Get Package Rating)\nconst handleGetPackageRating = async (id, headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        const ratingQuery = 'SELECT * FROM package_ratings WHERE package_id = $1';\n        const res = await dbService_1.default.query(ratingQuery, [id]);\n        if (res.rows.length === 0) {\n            return (0, response_1.sendResponse)(500, { message: 'The package rating system choked on at least one of the metrics.' });\n        }\n        const rating = res.rows[0];\n        return (0, response_1.sendResponse)(200, rating);\n    }\n    catch (error) {\n        console.error('Get Package Rating Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleGetPackageRating = handleGetPackageRating;\n// Handler for /package/{id}/cost - GET (Get Package Cost)\nconst handleGetPackageCost = async (id, headers, queryStringParameters) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    const dependency = queryStringParameters && queryStringParameters.dependency === 'true';\n    try {\n        // Fetch package and its dependencies\n        const costQuery = `\n      SELECT p.id, p.name, p.version, p.size_mb\n      FROM packages p\n      WHERE p.id = $1\n    `;\n        const costResult = await dbService_1.default.query(costQuery, [id]);\n        if (costResult.rows.length === 0) {\n            return (0, response_1.sendResponse)(404, { message: 'Package does not exist.' });\n        }\n        const packageCosts = {};\n        let totalCost = 0;\n        const stack = [id];\n        const visited = new Set();\n        while (stack.length > 0) {\n            const currentId = stack.pop();\n            if (visited.has(currentId))\n                continue;\n            visited.add(currentId);\n            const pkgQuery = 'SELECT * FROM packages WHERE id = $1';\n            const pkgResult = await dbService_1.default.query(pkgQuery, [currentId]);\n            if (pkgResult.rows.length === 0)\n                continue;\n            const pkg = pkgResult.rows[0];\n            packageCosts[pkg.id] = {\n                standaloneCost: pkg.size_mb,\n                totalCost: pkg.size_mb, // Will be updated if dependencies are included\n            };\n            totalCost += pkg.size_mb;\n            if (dependency) {\n                // Fetch dependencies from a hypothetical 'dependencies' table\n                const depQuery = 'SELECT dependency_id FROM dependencies WHERE package_id = $1';\n                const depResult = await dbService_1.default.query(depQuery, [currentId]);\n                depResult.rows.forEach(dep => {\n                    if (!visited.has(dep.dependency_id)) {\n                        stack.push(dep.dependency_id);\n                        totalCost += dep.size_mb; // Assuming size_mb is available\n                    }\n                });\n            }\n        }\n        if (dependency) {\n            // Sum up total costs including dependencies\n            let cumulativeCost = 0;\n            for (const pkgId in packageCosts) {\n                cumulativeCost += packageCosts[pkgId].standaloneCost || 0;\n                packageCosts[pkgId].totalCost = cumulativeCost;\n            }\n        }\n        else {\n            // Total cost is standalone\n            packageCosts[id].totalCost = packageCosts[id].standaloneCost || 0;\n        }\n        return (0, response_1.sendResponse)(200, packageCosts);\n    }\n    catch (error) {\n        console.error('Get Package Cost Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'Internal server error.' });\n    }\n};\nexports.handleGetPackageCost = handleGetPackageCost;\n// Handler for /tracks - GET (Get Planned Tracks)\nconst handleGetTracks = async (headers) => {\n    // Authenticate the request\n    let user;\n    try {\n        user = (0, auth_1.authenticate)(headers);\n    }\n    catch (err) {\n        return (0, response_1.sendResponse)(err.statusCode, { message: err.message });\n    }\n    try {\n        // Fetch planned tracks for the user\n        // Assuming a 'user_tracks' table mapping user_id to tracks\n        const tracksQuery = `\n      SELECT t.track_name\n      FROM user_tracks ut\n      JOIN tracks t ON ut.track_id = t.id\n      WHERE ut.user_id = $1\n    `;\n        const res = await dbService_1.default.query(tracksQuery, [user.sub]);\n        const plannedTracks = res.rows.map(row => row.track_name);\n        return (0, response_1.sendResponse)(200, { plannedTracks });\n    }\n    catch (error) {\n        console.error('Get Tracks Error:', error);\n        return (0, response_1.sendResponse)(500, { message: 'The system encountered an error while retrieving the student\\'s track information.' });\n    }\n};\nexports.handleGetTracks = handleGetTracks;\n\n\n//# sourceURL=webpack://lambda-api-handler/./handlers/handlers.ts?");

/***/ }),

/***/ "./index.ts":
/*!******************!*\
  !*** ./index.ts ***!
  \******************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

"use strict";
eval("\n// src/index.ts\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.handler = void 0;\nconst handlers_1 = __webpack_require__(/*! ./handlers/handlers */ \"./handlers/handlers.ts\");\nconst handler = async (event) => {\n    const { httpMethod, path, pathParameters, queryStringParameters, headers, body } = event;\n    console.log(`Received event: ${JSON.stringify(event)}`);\n    try {\n        // Routing logic based on path and method\n        if (path === '/authenticate' && httpMethod === 'PUT') {\n            return await (0, handlers_1.handleAuthenticate)(body || '{}');\n        }\n        else if (path === '/packages' && httpMethod === 'POST') {\n            return await (0, handlers_1.handleListPackages)(body || '[]', headers, queryStringParameters || {});\n        }\n        else if (path === '/reset' && httpMethod === 'DELETE') {\n            return await (0, handlers_1.handleResetRegistry)(headers);\n        }\n        else if (path === '/package/byRegEx' && httpMethod === 'POST') {\n            return await (0, handlers_1.handleSearchPackagesByRegEx)(body || '{}', headers);\n        }\n        else if (path && path.startsWith('/package/byName/') && httpMethod === 'GET') {\n            const name = path.split('/').pop() || '';\n            return await (0, handlers_1.handleGetPackageHistoryByName)(name, headers);\n        }\n        else if (path === '/package' && httpMethod === 'POST') {\n            return await (0, handlers_1.handleCreatePackage)(body || '{}', headers);\n        }\n        else if (path && path.startsWith('/package/') && path.endsWith('/rate') && httpMethod === 'GET') {\n            const id = path.split('/')[2];\n            return await (0, handlers_1.handleGetPackageRating)(id, headers);\n        }\n        else if (path && path.startsWith('/package/') && path.endsWith('/cost') && httpMethod === 'GET') {\n            const id = path.split('/')[2];\n            return await (0, handlers_1.handleGetPackageCost)(id, headers, queryStringParameters || {});\n        }\n        else if (path && path.startsWith('/package/') && httpMethod === 'GET') {\n            const id = path.split('/')[2];\n            return await (0, handlers_1.handleRetrievePackage)(id, headers);\n        }\n        else if (path && path.startsWith('/package/') && httpMethod === 'PUT') {\n            const id = path.split('/')[2];\n            return await (0, handlers_1.handleUpdatePackage)(id, body || '{}', headers);\n        }\n        else if (path && path.startsWith('/package/') && httpMethod === 'DELETE') {\n            const id = path.split('/')[2];\n            return await (0, handlers_1.handleDeletePackage)(id, headers);\n        }\n        else if (path === '/tracks' && httpMethod === 'GET') {\n            return await (0, handlers_1.handleGetTracks)(headers);\n        }\n        // If no route matched\n        return {\n            statusCode: 404,\n            body: JSON.stringify({ message: 'Endpoint not found.' }),\n            headers: {\n                'Content-Type': 'application/json',\n            },\n        };\n    }\n    catch (error) {\n        console.error('Lambda Handler Error:', error);\n        if (error.statusCode && error.message) {\n            return {\n                statusCode: error.statusCode,\n                body: JSON.stringify({ message: error.message }),\n                headers: {\n                    'Content-Type': 'application/json',\n                },\n            };\n        }\n        return {\n            statusCode: 500,\n            body: JSON.stringify({ message: 'Internal server error.' }),\n            headers: {\n                'Content-Type': 'application/json',\n            },\n        };\n    }\n};\nexports.handler = handler;\n\n\n//# sourceURL=webpack://lambda-api-handler/./index.ts?");

/***/ }),

/***/ "./services/dbService.ts":
/*!*******************************!*\
  !*** ./services/dbService.ts ***!
  \*******************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.createPackage = exports.getUserByName = void 0;\n// src/services/dbService.ts\nconst dotenv_1 = __importDefault(__webpack_require__(/*! dotenv */ \"dotenv\"));\nconst pg_1 = __webpack_require__(/*! pg */ \"pg\");\ndotenv_1.default.config();\n// Initialize PostgreSQL connection pool\nconst pool = new pg_1.Pool({\n    host: process.env.RDS_HOST,\n    user: process.env.RDS_USER,\n    password: process.env.RDS_PASSWORD,\n    database: process.env.RDS_DATABASE,\n    port: parseInt(process.env.RDS_PORT || '5432'),\n    max: 20,\n    idleTimeoutMillis: 30000,\n    connectionTimeoutMillis: 2000,\n    ssl: {\n        rejectUnauthorized: false // This line will fix new error\n    }\n});\n// Export pool for use in other modules\nexports[\"default\"] = pool;\n// DB Service Functions\nconst getUserByName = async (name) => {\n    const query = 'SELECT * FROM users WHERE name = $1';\n    const result = await pool.query(query, [name]);\n    if (result.rows.length === 0)\n        return null;\n    return result.rows[0];\n};\nexports.getUserByName = getUserByName;\nconst createPackage = async (metadata, data) => {\n    const insertText = `\n    INSERT INTO packages (id, name, version, content, url, debloat, js_program)\n    VALUES ($1, $2, $3, $4, $5, $6, $7)\n    RETURNING *;\n  `;\n    const insertValues = [\n        metadata.ID,\n        metadata.Name,\n        metadata.Version,\n        data.Content || null,\n        data.URL || null,\n        data.debloat || false,\n        data.JSProgram || null,\n    ];\n    const res = await pool.query(insertText, insertValues);\n    return res.rows[0];\n};\nexports.createPackage = createPackage;\n// Additional DB functions for other endpoints...\n\n\n//# sourceURL=webpack://lambda-api-handler/./services/dbService.ts?");

/***/ }),

/***/ "./services/s3Service.ts":
/*!*******************************!*\
  !*** ./services/s3Service.ts ***!
  \*******************************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\n// src/services/s3Service.ts\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.deletePackageContent = exports.getPackageContent = exports.uploadPackageContent = void 0;\nconst aws_sdk_1 = __importDefault(__webpack_require__(/*! aws-sdk */ \"aws-sdk\"));\nconst s3 = new aws_sdk_1.default.S3();\n// S3 Service Functions\nconst uploadPackageContent = async (packageId, content) => {\n    const params = {\n        Bucket: process.env.S3_BUCKET,\n        Key: `packages/${packageId}.zip`,\n        Body: Buffer.from(content, 'base64'),\n    };\n    await s3.putObject(params).promise();\n};\nexports.uploadPackageContent = uploadPackageContent;\nconst getPackageContent = async (packageId) => {\n    const params = {\n        Bucket: process.env.S3_BUCKET,\n        Key: `packages/${packageId}.zip`,\n    };\n    const data = await s3.getObject(params).promise();\n    return data.Body.toString('base64');\n};\nexports.getPackageContent = getPackageContent;\nconst deletePackageContent = async (packageId) => {\n    const params = {\n        Bucket: process.env.S3_BUCKET,\n        Key: `packages/${packageId}.zip`,\n    };\n    await s3.deleteObject(params).promise();\n};\nexports.deletePackageContent = deletePackageContent;\n// Additional S3 functions as needed...\n\n\n//# sourceURL=webpack://lambda-api-handler/./services/s3Service.ts?");

/***/ }),

/***/ "./utils/auth.ts":
/*!***********************!*\
  !*** ./utils/auth.ts ***!
  \***********************/
/***/ (function(__unused_webpack_module, exports, __webpack_require__) {

"use strict";
eval("\n// src/utils/auth.ts\nvar __importDefault = (this && this.__importDefault) || function (mod) {\n    return (mod && mod.__esModule) ? mod : { \"default\": mod };\n};\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.authenticate = void 0;\nconst jsonwebtoken_1 = __importDefault(__webpack_require__(/*! jsonwebtoken */ \"jsonwebtoken\"));\nconst authenticate = (headers) => {\n    const authHeader = headers['X-Authorization'] || headers['x-authorization'];\n    if (!authHeader) {\n        throw { statusCode: 403, message: 'Missing Authentication Token' };\n    }\n    const tokenParts = authHeader.split(' ');\n    if (tokenParts.length !== 2 || tokenParts[0].toLowerCase() !== 'bearer') {\n        throw { statusCode: 403, message: 'Invalid Authentication Token' };\n    }\n    const token = tokenParts[1];\n    try {\n        const decoded = jsonwebtoken_1.default.verify(token, process.env.JWT_SECRET);\n        return decoded;\n    }\n    catch (err) {\n        throw { statusCode: 403, message: 'Invalid Authentication Token' };\n    }\n};\nexports.authenticate = authenticate;\n\n\n//# sourceURL=webpack://lambda-api-handler/./utils/auth.ts?");

/***/ }),

/***/ "./utils/response.ts":
/*!***************************!*\
  !*** ./utils/response.ts ***!
  \***************************/
/***/ ((__unused_webpack_module, exports) => {

"use strict";
eval("\n// src/utils/response.ts\nObject.defineProperty(exports, \"__esModule\", ({ value: true }));\nexports.sendResponse = void 0;\nconst sendResponse = (statusCode, body) => {\n    return {\n        statusCode,\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(body),\n    };\n};\nexports.sendResponse = sendResponse;\n\n\n//# sourceURL=webpack://lambda-api-handler/./utils/response.ts?");

/***/ }),

/***/ "./rating/logger.ts":
/*!**************************!*\
  !*** ./rating/logger.ts ***!
  \**************************/
/***/ (() => {

eval("throw new Error(\"Module parse failed: Unexpected token (12:5)\\nYou may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders\\n| import \\\"dotenv/config\\\";\\n| \\n> type CustomLogger = {\\n|   info: winston.LeveledLogMethod;\\n|   debug: winston.LeveledLogMethod;\");\n\n//# sourceURL=webpack://lambda-api-handler/./rating/logger.ts?");

/***/ }),

/***/ "./rating/metrics/netScore.ts":
/*!************************************!*\
  !*** ./rating/metrics/netScore.ts ***!
  \************************************/
/***/ (() => {

eval("throw new Error(\"Module parse failed: Unexpected token (22:48)\\nYou may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders\\n|  * @param repoName The name of the repository\\n|  */\\n> export async function calculateNetScore(linkPath?: string, repoInfo?: GithubRepoInfo): Promise<any> {\\n|   let results: any = [];\\n|   if (linkPath) {\");\n\n//# sourceURL=webpack://lambda-api-handler/./rating/metrics/netScore.ts?");

/***/ }),

/***/ "./rating/processURL.ts":
/*!******************************!*\
  !*** ./rating/processURL.ts ***!
  \******************************/
/***/ (() => {

eval("throw new Error(\"Module parse failed: Unexpected token (16:39)\\nYou may need an appropriate loader to handle this file type, currently no loaders are configured to process this file. See https://webpack.js.org/concepts#loaders\\n|  * @returns The owner and package name of the GitHub repository\\n|  */\\n> export async function getGithubRepo(url: string): Promise<returnRepo | null> {\\n|   const { default: fetch } = await import(\\\"node-fetch\\\");\\n|   const trimmedUrl = url.trim();\");\n\n//# sourceURL=webpack://lambda-api-handler/./rating/processURL.ts?");

/***/ }),

/***/ "../../node_modules/adm-zip/adm-zip.js":
/*!*********************************************!*\
  !*** ../../node_modules/adm-zip/adm-zip.js ***!
  \*********************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("const Utils = __webpack_require__(/*! ./util */ \"../../node_modules/adm-zip/util/index.js\");\nconst pth = __webpack_require__(/*! path */ \"path\");\nconst ZipEntry = __webpack_require__(/*! ./zipEntry */ \"../../node_modules/adm-zip/zipEntry.js\");\nconst ZipFile = __webpack_require__(/*! ./zipFile */ \"../../node_modules/adm-zip/zipFile.js\");\n\nconst get_Bool = (...val) => Utils.findLast(val, (c) => typeof c === \"boolean\");\nconst get_Str = (...val) => Utils.findLast(val, (c) => typeof c === \"string\");\nconst get_Fun = (...val) => Utils.findLast(val, (c) => typeof c === \"function\");\n\nconst defaultOptions = {\n    // option \"noSort\" : if true it disables files sorting\n    noSort: false,\n    // read entries during load (initial loading may be slower)\n    readEntries: false,\n    // default method is none\n    method: Utils.Constants.NONE,\n    // file system\n    fs: null\n};\n\nmodule.exports = function (/**String*/ input, /** object */ options) {\n    let inBuffer = null;\n\n    // create object based default options, allowing them to be overwritten\n    const opts = Object.assign(Object.create(null), defaultOptions);\n\n    // test input variable\n    if (input && \"object\" === typeof input) {\n        // if value is not buffer we accept it to be object with options\n        if (!(input instanceof Uint8Array)) {\n            Object.assign(opts, input);\n            input = opts.input ? opts.input : undefined;\n            if (opts.input) delete opts.input;\n        }\n\n        // if input is buffer\n        if (Buffer.isBuffer(input)) {\n            inBuffer = input;\n            opts.method = Utils.Constants.BUFFER;\n            input = undefined;\n        }\n    }\n\n    // assign options\n    Object.assign(opts, options);\n\n    // instanciate utils filesystem\n    const filetools = new Utils(opts);\n\n    if (typeof opts.decoder !== \"object\" || typeof opts.decoder.encode !== \"function\" || typeof opts.decoder.decode !== \"function\") {\n        opts.decoder = Utils.decoder;\n    }\n\n    // if input is file name we retrieve its content\n    if (input && \"string\" === typeof input) {\n        // load zip file\n        if (filetools.fs.existsSync(input)) {\n            opts.method = Utils.Constants.FILE;\n            opts.filename = input;\n            inBuffer = filetools.fs.readFileSync(input);\n        } else {\n            throw Utils.Errors.INVALID_FILENAME();\n        }\n    }\n\n    // create variable\n    const _zip = new ZipFile(inBuffer, opts);\n\n    const { canonical, sanitize, zipnamefix } = Utils;\n\n    function getEntry(/**Object*/ entry) {\n        if (entry && _zip) {\n            var item;\n            // If entry was given as a file name\n            if (typeof entry === \"string\") item = _zip.getEntry(pth.posix.normalize(entry));\n            // if entry was given as a ZipEntry object\n            if (typeof entry === \"object\" && typeof entry.entryName !== \"undefined\" && typeof entry.header !== \"undefined\") item = _zip.getEntry(entry.entryName);\n\n            if (item) {\n                return item;\n            }\n        }\n        return null;\n    }\n\n    function fixPath(zipPath) {\n        const { join, normalize, sep } = pth.posix;\n        // convert windows file separators and normalize\n        return join(\".\", normalize(sep + zipPath.split(\"\\\\\").join(sep) + sep));\n    }\n\n    function filenameFilter(filterfn) {\n        if (filterfn instanceof RegExp) {\n            // if filter is RegExp wrap it\n            return (function (rx) {\n                return function (filename) {\n                    return rx.test(filename);\n                };\n            })(filterfn);\n        } else if (\"function\" !== typeof filterfn) {\n            // if filter is not function we will replace it\n            return () => true;\n        }\n        return filterfn;\n    }\n\n    // keep last character on folders\n    const relativePath = (local, entry) => {\n        let lastChar = entry.slice(-1);\n        lastChar = lastChar === filetools.sep ? filetools.sep : \"\";\n        return pth.relative(local, entry) + lastChar;\n    };\n\n    return {\n        /**\n         * Extracts the given entry from the archive and returns the content as a Buffer object\n         * @param {ZipEntry|string} entry ZipEntry object or String with the full path of the entry\n         * @param {Buffer|string} [pass] - password\n         * @return Buffer or Null in case of error\n         */\n        readFile: function (entry, pass) {\n            var item = getEntry(entry);\n            return (item && item.getData(pass)) || null;\n        },\n\n        /**\n         * Returns how many child elements has on entry (directories) on files it is always 0\n         * @param {ZipEntry|string} entry ZipEntry object or String with the full path of the entry\n         * @returns {integer}\n         */\n        childCount: function (entry) {\n            const item = getEntry(entry);\n            if (item) {\n                return _zip.getChildCount(item);\n            }\n        },\n\n        /**\n         * Asynchronous readFile\n         * @param {ZipEntry|string} entry ZipEntry object or String with the full path of the entry\n         * @param {callback} callback\n         *\n         * @return Buffer or Null in case of error\n         */\n        readFileAsync: function (entry, callback) {\n            var item = getEntry(entry);\n            if (item) {\n                item.getDataAsync(callback);\n            } else {\n                callback(null, \"getEntry failed for:\" + entry);\n            }\n        },\n\n        /**\n         * Extracts the given entry from the archive and returns the content as plain text in the given encoding\n         * @param {ZipEntry|string} entry - ZipEntry object or String with the full path of the entry\n         * @param {string} encoding - Optional. If no encoding is specified utf8 is used\n         *\n         * @return String\n         */\n        readAsText: function (entry, encoding) {\n            var item = getEntry(entry);\n            if (item) {\n                var data = item.getData();\n                if (data && data.length) {\n                    return data.toString(encoding || \"utf8\");\n                }\n            }\n            return \"\";\n        },\n\n        /**\n         * Asynchronous readAsText\n         * @param {ZipEntry|string} entry ZipEntry object or String with the full path of the entry\n         * @param {callback} callback\n         * @param {string} [encoding] - Optional. If no encoding is specified utf8 is used\n         *\n         * @return String\n         */\n        readAsTextAsync: function (entry, callback, encoding) {\n            var item = getEntry(entry);\n            if (item) {\n                item.getDataAsync(function (data, err) {\n                    if (err) {\n                        callback(data, err);\n                        return;\n                    }\n\n                    if (data && data.length) {\n                        callback(data.toString(encoding || \"utf8\"));\n                    } else {\n                        callback(\"\");\n                    }\n                });\n            } else {\n                callback(\"\");\n            }\n        },\n\n        /**\n         * Remove the entry from the file or the entry and all it's nested directories and files if the given entry is a directory\n         *\n         * @param {ZipEntry|string} entry\n         * @returns {void}\n         */\n        deleteFile: function (entry, withsubfolders = true) {\n            // @TODO: test deleteFile\n            var item = getEntry(entry);\n            if (item) {\n                _zip.deleteFile(item.entryName, withsubfolders);\n            }\n        },\n\n        /**\n         * Remove the entry from the file or directory without affecting any nested entries\n         *\n         * @param {ZipEntry|string} entry\n         * @returns {void}\n         */\n        deleteEntry: function (entry) {\n            // @TODO: test deleteEntry\n            var item = getEntry(entry);\n            if (item) {\n                _zip.deleteEntry(item.entryName);\n            }\n        },\n\n        /**\n         * Adds a comment to the zip. The zip must be rewritten after adding the comment.\n         *\n         * @param {string} comment\n         */\n        addZipComment: function (comment) {\n            // @TODO: test addZipComment\n            _zip.comment = comment;\n        },\n\n        /**\n         * Returns the zip comment\n         *\n         * @return String\n         */\n        getZipComment: function () {\n            return _zip.comment || \"\";\n        },\n\n        /**\n         * Adds a comment to a specified zipEntry. The zip must be rewritten after adding the comment\n         * The comment cannot exceed 65535 characters in length\n         *\n         * @param {ZipEntry} entry\n         * @param {string} comment\n         */\n        addZipEntryComment: function (entry, comment) {\n            var item = getEntry(entry);\n            if (item) {\n                item.comment = comment;\n            }\n        },\n\n        /**\n         * Returns the comment of the specified entry\n         *\n         * @param {ZipEntry} entry\n         * @return String\n         */\n        getZipEntryComment: function (entry) {\n            var item = getEntry(entry);\n            if (item) {\n                return item.comment || \"\";\n            }\n            return \"\";\n        },\n\n        /**\n         * Updates the content of an existing entry inside the archive. The zip must be rewritten after updating the content\n         *\n         * @param {ZipEntry} entry\n         * @param {Buffer} content\n         */\n        updateFile: function (entry, content) {\n            var item = getEntry(entry);\n            if (item) {\n                item.setData(content);\n            }\n        },\n\n        /**\n         * Adds a file from the disk to the archive\n         *\n         * @param {string} localPath File to add to zip\n         * @param {string} [zipPath] Optional path inside the zip\n         * @param {string} [zipName] Optional name for the file\n         * @param {string} [comment] Optional file comment\n         */\n        addLocalFile: function (localPath, zipPath, zipName, comment) {\n            if (filetools.fs.existsSync(localPath)) {\n                // fix ZipPath\n                zipPath = zipPath ? fixPath(zipPath) : \"\";\n\n                // p - local file name\n                const p = pth.win32.basename(pth.win32.normalize(localPath));\n\n                // add file name into zippath\n                zipPath += zipName ? zipName : p;\n\n                // read file attributes\n                const _attr = filetools.fs.statSync(localPath);\n\n                // get file content\n                const data = _attr.isFile() ? filetools.fs.readFileSync(localPath) : Buffer.alloc(0);\n\n                // if folder\n                if (_attr.isDirectory()) zipPath += filetools.sep;\n\n                // add file into zip file\n                this.addFile(zipPath, data, comment, _attr);\n            } else {\n                throw Utils.Errors.FILE_NOT_FOUND(localPath);\n            }\n        },\n\n        /**\n         * Callback for showing if everything was done.\n         *\n         * @callback doneCallback\n         * @param {Error} err - Error object\n         * @param {boolean} done - was request fully completed\n         */\n\n        /**\n         * Adds a file from the disk to the archive\n         *\n         * @param {(object|string)} options - options object, if it is string it us used as localPath.\n         * @param {string} options.localPath - Local path to the file.\n         * @param {string} [options.comment] - Optional file comment.\n         * @param {string} [options.zipPath] - Optional path inside the zip\n         * @param {string} [options.zipName] - Optional name for the file\n         * @param {doneCallback} callback - The callback that handles the response.\n         */\n        addLocalFileAsync: function (options, callback) {\n            options = typeof options === \"object\" ? options : { localPath: options };\n            const localPath = pth.resolve(options.localPath);\n            const { comment } = options;\n            let { zipPath, zipName } = options;\n            const self = this;\n\n            filetools.fs.stat(localPath, function (err, stats) {\n                if (err) return callback(err, false);\n                // fix ZipPath\n                zipPath = zipPath ? fixPath(zipPath) : \"\";\n                // p - local file name\n                const p = pth.win32.basename(pth.win32.normalize(localPath));\n                // add file name into zippath\n                zipPath += zipName ? zipName : p;\n\n                if (stats.isFile()) {\n                    filetools.fs.readFile(localPath, function (err, data) {\n                        if (err) return callback(err, false);\n                        self.addFile(zipPath, data, comment, stats);\n                        return setImmediate(callback, undefined, true);\n                    });\n                } else if (stats.isDirectory()) {\n                    zipPath += filetools.sep;\n                    self.addFile(zipPath, Buffer.alloc(0), comment, stats);\n                    return setImmediate(callback, undefined, true);\n                }\n            });\n        },\n\n        /**\n         * Adds a local directory and all its nested files and directories to the archive\n         *\n         * @param {string} localPath - local path to the folder\n         * @param {string} [zipPath] - optional path inside zip\n         * @param {(RegExp|function)} [filter] - optional RegExp or Function if files match will be included.\n         */\n        addLocalFolder: function (localPath, zipPath, filter) {\n            // Prepare filter\n            filter = filenameFilter(filter);\n\n            // fix ZipPath\n            zipPath = zipPath ? fixPath(zipPath) : \"\";\n\n            // normalize the path first\n            localPath = pth.normalize(localPath);\n\n            if (filetools.fs.existsSync(localPath)) {\n                const items = filetools.findFiles(localPath);\n                const self = this;\n\n                if (items.length) {\n                    for (const filepath of items) {\n                        const p = pth.join(zipPath, relativePath(localPath, filepath));\n                        if (filter(p)) {\n                            self.addLocalFile(filepath, pth.dirname(p));\n                        }\n                    }\n                }\n            } else {\n                throw Utils.Errors.FILE_NOT_FOUND(localPath);\n            }\n        },\n\n        /**\n         * Asynchronous addLocalFolder\n         * @param {string} localPath\n         * @param {callback} callback\n         * @param {string} [zipPath] optional path inside zip\n         * @param {RegExp|function} [filter] optional RegExp or Function if files match will\n         *               be included.\n         */\n        addLocalFolderAsync: function (localPath, callback, zipPath, filter) {\n            // Prepare filter\n            filter = filenameFilter(filter);\n\n            // fix ZipPath\n            zipPath = zipPath ? fixPath(zipPath) : \"\";\n\n            // normalize the path first\n            localPath = pth.normalize(localPath);\n\n            var self = this;\n            filetools.fs.open(localPath, \"r\", function (err) {\n                if (err && err.code === \"ENOENT\") {\n                    callback(undefined, Utils.Errors.FILE_NOT_FOUND(localPath));\n                } else if (err) {\n                    callback(undefined, err);\n                } else {\n                    var items = filetools.findFiles(localPath);\n                    var i = -1;\n\n                    var next = function () {\n                        i += 1;\n                        if (i < items.length) {\n                            var filepath = items[i];\n                            var p = relativePath(localPath, filepath).split(\"\\\\\").join(\"/\"); //windows fix\n                            p = p\n                                .normalize(\"NFD\")\n                                .replace(/[\\u0300-\\u036f]/g, \"\")\n                                .replace(/[^\\x20-\\x7E]/g, \"\"); // accent fix\n                            if (filter(p)) {\n                                filetools.fs.stat(filepath, function (er0, stats) {\n                                    if (er0) callback(undefined, er0);\n                                    if (stats.isFile()) {\n                                        filetools.fs.readFile(filepath, function (er1, data) {\n                                            if (er1) {\n                                                callback(undefined, er1);\n                                            } else {\n                                                self.addFile(zipPath + p, data, \"\", stats);\n                                                next();\n                                            }\n                                        });\n                                    } else {\n                                        self.addFile(zipPath + p + \"/\", Buffer.alloc(0), \"\", stats);\n                                        next();\n                                    }\n                                });\n                            } else {\n                                process.nextTick(() => {\n                                    next();\n                                });\n                            }\n                        } else {\n                            callback(true, undefined);\n                        }\n                    };\n\n                    next();\n                }\n            });\n        },\n\n        /**\n         * Adds a local directory and all its nested files and directories to the archive\n         *\n         * @param {object | string} options - options object, if it is string it us used as localPath.\n         * @param {string} options.localPath - Local path to the folder.\n         * @param {string} [options.zipPath] - optional path inside zip.\n         * @param {RegExp|function} [options.filter] - optional RegExp or Function if files match will be included.\n         * @param {function|string} [options.namefix] - optional function to help fix filename\n         * @param {doneCallback} callback - The callback that handles the response.\n         *\n         */\n        addLocalFolderAsync2: function (options, callback) {\n            const self = this;\n            options = typeof options === \"object\" ? options : { localPath: options };\n            localPath = pth.resolve(fixPath(options.localPath));\n            let { zipPath, filter, namefix } = options;\n\n            if (filter instanceof RegExp) {\n                filter = (function (rx) {\n                    return function (filename) {\n                        return rx.test(filename);\n                    };\n                })(filter);\n            } else if (\"function\" !== typeof filter) {\n                filter = function () {\n                    return true;\n                };\n            }\n\n            // fix ZipPath\n            zipPath = zipPath ? fixPath(zipPath) : \"\";\n\n            // Check Namefix function\n            if (namefix == \"latin1\") {\n                namefix = (str) =>\n                    str\n                        .normalize(\"NFD\")\n                        .replace(/[\\u0300-\\u036f]/g, \"\")\n                        .replace(/[^\\x20-\\x7E]/g, \"\"); // accent fix (latin1 characers only)\n            }\n\n            if (typeof namefix !== \"function\") namefix = (str) => str;\n\n            // internal, create relative path + fix the name\n            const relPathFix = (entry) => pth.join(zipPath, namefix(relativePath(localPath, entry)));\n            const fileNameFix = (entry) => pth.win32.basename(pth.win32.normalize(namefix(entry)));\n\n            filetools.fs.open(localPath, \"r\", function (err) {\n                if (err && err.code === \"ENOENT\") {\n                    callback(undefined, Utils.Errors.FILE_NOT_FOUND(localPath));\n                } else if (err) {\n                    callback(undefined, err);\n                } else {\n                    filetools.findFilesAsync(localPath, function (err, fileEntries) {\n                        if (err) return callback(err);\n                        fileEntries = fileEntries.filter((dir) => filter(relPathFix(dir)));\n                        if (!fileEntries.length) callback(undefined, false);\n\n                        setImmediate(\n                            fileEntries.reverse().reduce(function (next, entry) {\n                                return function (err, done) {\n                                    if (err || done === false) return setImmediate(next, err, false);\n\n                                    self.addLocalFileAsync(\n                                        {\n                                            localPath: entry,\n                                            zipPath: pth.dirname(relPathFix(entry)),\n                                            zipName: fileNameFix(entry)\n                                        },\n                                        next\n                                    );\n                                };\n                            }, callback)\n                        );\n                    });\n                }\n            });\n        },\n\n        /**\n         * Adds a local directory and all its nested files and directories to the archive\n         *\n         * @param {string} localPath - path where files will be extracted\n         * @param {object} props - optional properties\n         * @param {string} [props.zipPath] - optional path inside zip\n         * @param {RegExp|function} [props.filter] - optional RegExp or Function if files match will be included.\n         * @param {function|string} [props.namefix] - optional function to help fix filename\n         */\n        addLocalFolderPromise: function (localPath, props) {\n            return new Promise((resolve, reject) => {\n                this.addLocalFolderAsync2(Object.assign({ localPath }, props), (err, done) => {\n                    if (err) reject(err);\n                    if (done) resolve(this);\n                });\n            });\n        },\n\n        /**\n         * Allows you to create a entry (file or directory) in the zip file.\n         * If you want to create a directory the entryName must end in / and a null buffer should be provided.\n         * Comment and attributes are optional\n         *\n         * @param {string} entryName\n         * @param {Buffer | string} content - file content as buffer or utf8 coded string\n         * @param {string} [comment] - file comment\n         * @param {number | object} [attr] - number as unix file permissions, object as filesystem Stats object\n         */\n        addFile: function (entryName, content, comment, attr) {\n            entryName = zipnamefix(entryName);\n            let entry = getEntry(entryName);\n            const update = entry != null;\n\n            // prepare new entry\n            if (!update) {\n                entry = new ZipEntry(opts);\n                entry.entryName = entryName;\n            }\n            entry.comment = comment || \"\";\n\n            const isStat = \"object\" === typeof attr && attr instanceof filetools.fs.Stats;\n\n            // last modification time from file stats\n            if (isStat) {\n                entry.header.time = attr.mtime;\n            }\n\n            // Set file attribute\n            var fileattr = entry.isDirectory ? 0x10 : 0; // (MS-DOS directory flag)\n\n            // extended attributes field for Unix\n            // set file type either S_IFDIR / S_IFREG\n            let unix = entry.isDirectory ? 0x4000 : 0x8000;\n\n            if (isStat) {\n                // File attributes from file stats\n                unix |= 0xfff & attr.mode;\n            } else if (\"number\" === typeof attr) {\n                // attr from given attr values\n                unix |= 0xfff & attr;\n            } else {\n                // Default values:\n                unix |= entry.isDirectory ? 0o755 : 0o644; // permissions (drwxr-xr-x) or (-r-wr--r--)\n            }\n\n            fileattr = (fileattr | (unix << 16)) >>> 0; // add attributes\n\n            entry.attr = fileattr;\n\n            entry.setData(content);\n            if (!update) _zip.setEntry(entry);\n\n            return entry;\n        },\n\n        /**\n         * Returns an array of ZipEntry objects representing the files and folders inside the archive\n         *\n         * @param {string} [password]\n         * @returns Array\n         */\n        getEntries: function (password) {\n            _zip.password = password;\n            return _zip ? _zip.entries : [];\n        },\n\n        /**\n         * Returns a ZipEntry object representing the file or folder specified by ``name``.\n         *\n         * @param {string} name\n         * @return ZipEntry\n         */\n        getEntry: function (/**String*/ name) {\n            return getEntry(name);\n        },\n\n        getEntryCount: function () {\n            return _zip.getEntryCount();\n        },\n\n        forEach: function (callback) {\n            return _zip.forEach(callback);\n        },\n\n        /**\n         * Extracts the given entry to the given targetPath\n         * If the entry is a directory inside the archive, the entire directory and it's subdirectories will be extracted\n         *\n         * @param {string|ZipEntry} entry - ZipEntry object or String with the full path of the entry\n         * @param {string} targetPath - Target folder where to write the file\n         * @param {boolean} [maintainEntryPath=true] - If maintainEntryPath is true and the entry is inside a folder, the entry folder will be created in targetPath as well. Default is TRUE\n         * @param {boolean} [overwrite=false] - If the file already exists at the target path, the file will be overwriten if this is true.\n         * @param {boolean} [keepOriginalPermission=false] - The file will be set as the permission from the entry if this is true.\n         * @param {string} [outFileName] - String If set will override the filename of the extracted file (Only works if the entry is a file)\n         *\n         * @return Boolean\n         */\n        extractEntryTo: function (entry, targetPath, maintainEntryPath, overwrite, keepOriginalPermission, outFileName) {\n            overwrite = get_Bool(false, overwrite);\n            keepOriginalPermission = get_Bool(false, keepOriginalPermission);\n            maintainEntryPath = get_Bool(true, maintainEntryPath);\n            outFileName = get_Str(keepOriginalPermission, outFileName);\n\n            var item = getEntry(entry);\n            if (!item) {\n                throw Utils.Errors.NO_ENTRY();\n            }\n\n            var entryName = canonical(item.entryName);\n\n            var target = sanitize(targetPath, outFileName && !item.isDirectory ? outFileName : maintainEntryPath ? entryName : pth.basename(entryName));\n\n            if (item.isDirectory) {\n                var children = _zip.getEntryChildren(item);\n                children.forEach(function (child) {\n                    if (child.isDirectory) return;\n                    var content = child.getData();\n                    if (!content) {\n                        throw Utils.Errors.CANT_EXTRACT_FILE();\n                    }\n                    var name = canonical(child.entryName);\n                    var childName = sanitize(targetPath, maintainEntryPath ? name : pth.basename(name));\n                    // The reverse operation for attr depend on method addFile()\n                    const fileAttr = keepOriginalPermission ? child.header.fileAttr : undefined;\n                    filetools.writeFileTo(childName, content, overwrite, fileAttr);\n                });\n                return true;\n            }\n\n            var content = item.getData(_zip.password);\n            if (!content) throw Utils.Errors.CANT_EXTRACT_FILE();\n\n            if (filetools.fs.existsSync(target) && !overwrite) {\n                throw Utils.Errors.CANT_OVERRIDE();\n            }\n            // The reverse operation for attr depend on method addFile()\n            const fileAttr = keepOriginalPermission ? entry.header.fileAttr : undefined;\n            filetools.writeFileTo(target, content, overwrite, fileAttr);\n\n            return true;\n        },\n\n        /**\n         * Test the archive\n         * @param {string} [pass]\n         */\n        test: function (pass) {\n            if (!_zip) {\n                return false;\n            }\n\n            for (var entry in _zip.entries) {\n                try {\n                    if (entry.isDirectory) {\n                        continue;\n                    }\n                    var content = _zip.entries[entry].getData(pass);\n                    if (!content) {\n                        return false;\n                    }\n                } catch (err) {\n                    return false;\n                }\n            }\n            return true;\n        },\n\n        /**\n         * Extracts the entire archive to the given location\n         *\n         * @param {string} targetPath Target location\n         * @param {boolean} [overwrite=false] If the file already exists at the target path, the file will be overwriten if this is true.\n         *                  Default is FALSE\n         * @param {boolean} [keepOriginalPermission=false] The file will be set as the permission from the entry if this is true.\n         *                  Default is FALSE\n         * @param {string|Buffer} [pass] password\n         */\n        extractAllTo: function (targetPath, overwrite, keepOriginalPermission, pass) {\n            keepOriginalPermission = get_Bool(false, keepOriginalPermission);\n            pass = get_Str(keepOriginalPermission, pass);\n            overwrite = get_Bool(false, overwrite);\n            if (!_zip) throw Utils.Errors.NO_ZIP();\n\n            _zip.entries.forEach(function (entry) {\n                var entryName = sanitize(targetPath, canonical(entry.entryName));\n                if (entry.isDirectory) {\n                    filetools.makeDir(entryName);\n                    return;\n                }\n                var content = entry.getData(pass);\n                if (!content) {\n                    throw Utils.Errors.CANT_EXTRACT_FILE();\n                }\n                // The reverse operation for attr depend on method addFile()\n                const fileAttr = keepOriginalPermission ? entry.header.fileAttr : undefined;\n                filetools.writeFileTo(entryName, content, overwrite, fileAttr);\n                try {\n                    filetools.fs.utimesSync(entryName, entry.header.time, entry.header.time);\n                } catch (err) {\n                    throw Utils.Errors.CANT_EXTRACT_FILE();\n                }\n            });\n        },\n\n        /**\n         * Asynchronous extractAllTo\n         *\n         * @param {string} targetPath Target location\n         * @param {boolean} [overwrite=false] If the file already exists at the target path, the file will be overwriten if this is true.\n         *                  Default is FALSE\n         * @param {boolean} [keepOriginalPermission=false] The file will be set as the permission from the entry if this is true.\n         *                  Default is FALSE\n         * @param {function} callback The callback will be executed when all entries are extracted successfully or any error is thrown.\n         */\n        extractAllToAsync: function (targetPath, overwrite, keepOriginalPermission, callback) {\n            callback = get_Fun(overwrite, keepOriginalPermission, callback);\n            keepOriginalPermission = get_Bool(false, keepOriginalPermission);\n            overwrite = get_Bool(false, overwrite);\n            if (!callback) {\n                return new Promise((resolve, reject) => {\n                    this.extractAllToAsync(targetPath, overwrite, keepOriginalPermission, function (err) {\n                        if (err) {\n                            reject(err);\n                        } else {\n                            resolve(this);\n                        }\n                    });\n                });\n            }\n            if (!_zip) {\n                callback(Utils.Errors.NO_ZIP());\n                return;\n            }\n\n            targetPath = pth.resolve(targetPath);\n            // convert entryName to\n            const getPath = (entry) => sanitize(targetPath, pth.normalize(canonical(entry.entryName)));\n            const getError = (msg, file) => new Error(msg + ': \"' + file + '\"');\n\n            // separate directories from files\n            const dirEntries = [];\n            const fileEntries = [];\n            _zip.entries.forEach((e) => {\n                if (e.isDirectory) {\n                    dirEntries.push(e);\n                } else {\n                    fileEntries.push(e);\n                }\n            });\n\n            // Create directory entries first synchronously\n            // this prevents race condition and assures folders are there before writing files\n            for (const entry of dirEntries) {\n                const dirPath = getPath(entry);\n                // The reverse operation for attr depend on method addFile()\n                const dirAttr = keepOriginalPermission ? entry.header.fileAttr : undefined;\n                try {\n                    filetools.makeDir(dirPath);\n                    if (dirAttr) filetools.fs.chmodSync(dirPath, dirAttr);\n                    // in unix timestamp will change if files are later added to folder, but still\n                    filetools.fs.utimesSync(dirPath, entry.header.time, entry.header.time);\n                } catch (er) {\n                    callback(getError(\"Unable to create folder\", dirPath));\n                }\n            }\n\n            fileEntries.reverse().reduce(function (next, entry) {\n                return function (err) {\n                    if (err) {\n                        next(err);\n                    } else {\n                        const entryName = pth.normalize(canonical(entry.entryName));\n                        const filePath = sanitize(targetPath, entryName);\n                        entry.getDataAsync(function (content, err_1) {\n                            if (err_1) {\n                                next(err_1);\n                            } else if (!content) {\n                                next(Utils.Errors.CANT_EXTRACT_FILE());\n                            } else {\n                                // The reverse operation for attr depend on method addFile()\n                                const fileAttr = keepOriginalPermission ? entry.header.fileAttr : undefined;\n                                filetools.writeFileToAsync(filePath, content, overwrite, fileAttr, function (succ) {\n                                    if (!succ) {\n                                        next(getError(\"Unable to write file\", filePath));\n                                    }\n                                    filetools.fs.utimes(filePath, entry.header.time, entry.header.time, function (err_2) {\n                                        if (err_2) {\n                                            next(getError(\"Unable to set times\", filePath));\n                                        } else {\n                                            next();\n                                        }\n                                    });\n                                });\n                            }\n                        });\n                    }\n                };\n            }, callback)();\n        },\n\n        /**\n         * Writes the newly created zip file to disk at the specified location or if a zip was opened and no ``targetFileName`` is provided, it will overwrite the opened zip\n         *\n         * @param {string} targetFileName\n         * @param {function} callback\n         */\n        writeZip: function (targetFileName, callback) {\n            if (arguments.length === 1) {\n                if (typeof targetFileName === \"function\") {\n                    callback = targetFileName;\n                    targetFileName = \"\";\n                }\n            }\n\n            if (!targetFileName && opts.filename) {\n                targetFileName = opts.filename;\n            }\n            if (!targetFileName) return;\n\n            var zipData = _zip.compressToBuffer();\n            if (zipData) {\n                var ok = filetools.writeFileTo(targetFileName, zipData, true);\n                if (typeof callback === \"function\") callback(!ok ? new Error(\"failed\") : null, \"\");\n            }\n        },\n\n        /**\n         *\n         * @param {string} targetFileName\n         * @param {object} [props]\n         * @param {boolean} [props.overwrite=true] If the file already exists at the target path, the file will be overwriten if this is true.\n         * @param {boolean} [props.perm] The file will be set as the permission from the entry if this is true.\n\n         * @returns {Promise<void>}\n         */\n        writeZipPromise: function (/**String*/ targetFileName, /* object */ props) {\n            const { overwrite, perm } = Object.assign({ overwrite: true }, props);\n\n            return new Promise((resolve, reject) => {\n                // find file name\n                if (!targetFileName && opts.filename) targetFileName = opts.filename;\n                if (!targetFileName) reject(\"ADM-ZIP: ZIP File Name Missing\");\n\n                this.toBufferPromise().then((zipData) => {\n                    const ret = (done) => (done ? resolve(done) : reject(\"ADM-ZIP: Wasn't able to write zip file\"));\n                    filetools.writeFileToAsync(targetFileName, zipData, overwrite, perm, ret);\n                }, reject);\n            });\n        },\n\n        /**\n         * @returns {Promise<Buffer>} A promise to the Buffer.\n         */\n        toBufferPromise: function () {\n            return new Promise((resolve, reject) => {\n                _zip.toAsyncBuffer(resolve, reject);\n            });\n        },\n\n        /**\n         * Returns the content of the entire zip file as a Buffer object\n         *\n         * @prop {function} [onSuccess]\n         * @prop {function} [onFail]\n         * @prop {function} [onItemStart]\n         * @prop {function} [onItemEnd]\n         * @returns {Buffer}\n         */\n        toBuffer: function (onSuccess, onFail, onItemStart, onItemEnd) {\n            if (typeof onSuccess === \"function\") {\n                _zip.toAsyncBuffer(onSuccess, onFail, onItemStart, onItemEnd);\n                return null;\n            }\n            return _zip.compressToBuffer();\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/adm-zip.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/headers/entryHeader.js":
/*!*********************************************************!*\
  !*** ../../node_modules/adm-zip/headers/entryHeader.js ***!
  \*********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("var Utils = __webpack_require__(/*! ../util */ \"../../node_modules/adm-zip/util/index.js\"),\n    Constants = Utils.Constants;\n\n/* The central directory file header */\nmodule.exports = function () {\n    var _verMade = 20, // v2.0\n        _version = 10, // v1.0\n        _flags = 0,\n        _method = 0,\n        _time = 0,\n        _crc = 0,\n        _compressedSize = 0,\n        _size = 0,\n        _fnameLen = 0,\n        _extraLen = 0,\n        _comLen = 0,\n        _diskStart = 0,\n        _inattr = 0,\n        _attr = 0,\n        _offset = 0;\n\n    _verMade |= Utils.isWin ? 0x0a00 : 0x0300;\n\n    // Set EFS flag since filename and comment fields are all by default encoded using UTF-8.\n    // Without it file names may be corrupted for other apps when file names use unicode chars\n    _flags |= Constants.FLG_EFS;\n\n    const _localHeader = {\n        extraLen: 0\n    };\n\n    // casting\n    const uint32 = (val) => Math.max(0, val) >>> 0;\n    const uint16 = (val) => Math.max(0, val) & 0xffff;\n    const uint8 = (val) => Math.max(0, val) & 0xff;\n\n    _time = Utils.fromDate2DOS(new Date());\n\n    return {\n        get made() {\n            return _verMade;\n        },\n        set made(val) {\n            _verMade = val;\n        },\n\n        get version() {\n            return _version;\n        },\n        set version(val) {\n            _version = val;\n        },\n\n        get flags() {\n            return _flags;\n        },\n        set flags(val) {\n            _flags = val;\n        },\n\n        get flags_efs() {\n            return (_flags & Constants.FLG_EFS) > 0;\n        },\n        set flags_efs(val) {\n            if (val) {\n                _flags |= Constants.FLG_EFS;\n            } else {\n                _flags &= ~Constants.FLG_EFS;\n            }\n        },\n\n        get flags_desc() {\n            return (_flags & Constants.FLG_DESC) > 0;\n        },\n        set flags_desc(val) {\n            if (val) {\n                _flags |= Constants.FLG_DESC;\n            } else {\n                _flags &= ~Constants.FLG_DESC;\n            }\n        },\n\n        get method() {\n            return _method;\n        },\n        set method(val) {\n            switch (val) {\n                case Constants.STORED:\n                    this.version = 10;\n                case Constants.DEFLATED:\n                default:\n                    this.version = 20;\n            }\n            _method = val;\n        },\n\n        get time() {\n            return Utils.fromDOS2Date(this.timeval);\n        },\n        set time(val) {\n            this.timeval = Utils.fromDate2DOS(val);\n        },\n\n        get timeval() {\n            return _time;\n        },\n        set timeval(val) {\n            _time = uint32(val);\n        },\n\n        get timeHighByte() {\n            return uint8(_time >>> 8);\n        },\n        get crc() {\n            return _crc;\n        },\n        set crc(val) {\n            _crc = uint32(val);\n        },\n\n        get compressedSize() {\n            return _compressedSize;\n        },\n        set compressedSize(val) {\n            _compressedSize = uint32(val);\n        },\n\n        get size() {\n            return _size;\n        },\n        set size(val) {\n            _size = uint32(val);\n        },\n\n        get fileNameLength() {\n            return _fnameLen;\n        },\n        set fileNameLength(val) {\n            _fnameLen = val;\n        },\n\n        get extraLength() {\n            return _extraLen;\n        },\n        set extraLength(val) {\n            _extraLen = val;\n        },\n\n        get extraLocalLength() {\n            return _localHeader.extraLen;\n        },\n        set extraLocalLength(val) {\n            _localHeader.extraLen = val;\n        },\n\n        get commentLength() {\n            return _comLen;\n        },\n        set commentLength(val) {\n            _comLen = val;\n        },\n\n        get diskNumStart() {\n            return _diskStart;\n        },\n        set diskNumStart(val) {\n            _diskStart = uint32(val);\n        },\n\n        get inAttr() {\n            return _inattr;\n        },\n        set inAttr(val) {\n            _inattr = uint32(val);\n        },\n\n        get attr() {\n            return _attr;\n        },\n        set attr(val) {\n            _attr = uint32(val);\n        },\n\n        // get Unix file permissions\n        get fileAttr() {\n            return (_attr || 0) >> 16 & 0xfff;\n        },\n\n        get offset() {\n            return _offset;\n        },\n        set offset(val) {\n            _offset = uint32(val);\n        },\n\n        get encrypted() {\n            return (_flags & Constants.FLG_ENC) === Constants.FLG_ENC;\n        },\n\n        get centralHeaderSize() {\n            return Constants.CENHDR + _fnameLen + _extraLen + _comLen;\n        },\n\n        get realDataOffset() {\n            return _offset + Constants.LOCHDR + _localHeader.fnameLen + _localHeader.extraLen;\n        },\n\n        get localHeader() {\n            return _localHeader;\n        },\n\n        loadLocalHeaderFromBinary: function (/*Buffer*/ input) {\n            var data = input.slice(_offset, _offset + Constants.LOCHDR);\n            // 30 bytes and should start with \"PK\\003\\004\"\n            if (data.readUInt32LE(0) !== Constants.LOCSIG) {\n                throw Utils.Errors.INVALID_LOC();\n            }\n\n            // version needed to extract\n            _localHeader.version = data.readUInt16LE(Constants.LOCVER);\n            // general purpose bit flag\n            _localHeader.flags = data.readUInt16LE(Constants.LOCFLG);\n            // compression method\n            _localHeader.method = data.readUInt16LE(Constants.LOCHOW);\n            // modification time (2 bytes time, 2 bytes date)\n            _localHeader.time = data.readUInt32LE(Constants.LOCTIM);\n            // uncompressed file crc-32 valu\n            _localHeader.crc = data.readUInt32LE(Constants.LOCCRC);\n            // compressed size\n            _localHeader.compressedSize = data.readUInt32LE(Constants.LOCSIZ);\n            // uncompressed size\n            _localHeader.size = data.readUInt32LE(Constants.LOCLEN);\n            // filename length\n            _localHeader.fnameLen = data.readUInt16LE(Constants.LOCNAM);\n            // extra field length\n            _localHeader.extraLen = data.readUInt16LE(Constants.LOCEXT);\n\n            // read extra data\n            const extraStart = _offset + Constants.LOCHDR + _localHeader.fnameLen;\n            const extraEnd = extraStart + _localHeader.extraLen;\n            return input.slice(extraStart, extraEnd);\n        },\n\n        loadFromBinary: function (/*Buffer*/ data) {\n            // data should be 46 bytes and start with \"PK 01 02\"\n            if (data.length !== Constants.CENHDR || data.readUInt32LE(0) !== Constants.CENSIG) {\n                throw Utils.Errors.INVALID_CEN();\n            }\n            // version made by\n            _verMade = data.readUInt16LE(Constants.CENVEM);\n            // version needed to extract\n            _version = data.readUInt16LE(Constants.CENVER);\n            // encrypt, decrypt flags\n            _flags = data.readUInt16LE(Constants.CENFLG);\n            // compression method\n            _method = data.readUInt16LE(Constants.CENHOW);\n            // modification time (2 bytes time, 2 bytes date)\n            _time = data.readUInt32LE(Constants.CENTIM);\n            // uncompressed file crc-32 value\n            _crc = data.readUInt32LE(Constants.CENCRC);\n            // compressed size\n            _compressedSize = data.readUInt32LE(Constants.CENSIZ);\n            // uncompressed size\n            _size = data.readUInt32LE(Constants.CENLEN);\n            // filename length\n            _fnameLen = data.readUInt16LE(Constants.CENNAM);\n            // extra field length\n            _extraLen = data.readUInt16LE(Constants.CENEXT);\n            // file comment length\n            _comLen = data.readUInt16LE(Constants.CENCOM);\n            // volume number start\n            _diskStart = data.readUInt16LE(Constants.CENDSK);\n            // internal file attributes\n            _inattr = data.readUInt16LE(Constants.CENATT);\n            // external file attributes\n            _attr = data.readUInt32LE(Constants.CENATX);\n            // LOC header offset\n            _offset = data.readUInt32LE(Constants.CENOFF);\n        },\n\n        localHeaderToBinary: function () {\n            // LOC header size (30 bytes)\n            var data = Buffer.alloc(Constants.LOCHDR);\n            // \"PK\\003\\004\"\n            data.writeUInt32LE(Constants.LOCSIG, 0);\n            // version needed to extract\n            data.writeUInt16LE(_version, Constants.LOCVER);\n            // general purpose bit flag\n            data.writeUInt16LE(_flags, Constants.LOCFLG);\n            // compression method\n            data.writeUInt16LE(_method, Constants.LOCHOW);\n            // modification time (2 bytes time, 2 bytes date)\n            data.writeUInt32LE(_time, Constants.LOCTIM);\n            // uncompressed file crc-32 value\n            data.writeUInt32LE(_crc, Constants.LOCCRC);\n            // compressed size\n            data.writeUInt32LE(_compressedSize, Constants.LOCSIZ);\n            // uncompressed size\n            data.writeUInt32LE(_size, Constants.LOCLEN);\n            // filename length\n            data.writeUInt16LE(_fnameLen, Constants.LOCNAM);\n            // extra field length\n            data.writeUInt16LE(_localHeader.extraLen, Constants.LOCEXT);\n            return data;\n        },\n\n        centralHeaderToBinary: function () {\n            // CEN header size (46 bytes)\n            var data = Buffer.alloc(Constants.CENHDR + _fnameLen + _extraLen + _comLen);\n            // \"PK\\001\\002\"\n            data.writeUInt32LE(Constants.CENSIG, 0);\n            // version made by\n            data.writeUInt16LE(_verMade, Constants.CENVEM);\n            // version needed to extract\n            data.writeUInt16LE(_version, Constants.CENVER);\n            // encrypt, decrypt flags\n            data.writeUInt16LE(_flags, Constants.CENFLG);\n            // compression method\n            data.writeUInt16LE(_method, Constants.CENHOW);\n            // modification time (2 bytes time, 2 bytes date)\n            data.writeUInt32LE(_time, Constants.CENTIM);\n            // uncompressed file crc-32 value\n            data.writeUInt32LE(_crc, Constants.CENCRC);\n            // compressed size\n            data.writeUInt32LE(_compressedSize, Constants.CENSIZ);\n            // uncompressed size\n            data.writeUInt32LE(_size, Constants.CENLEN);\n            // filename length\n            data.writeUInt16LE(_fnameLen, Constants.CENNAM);\n            // extra field length\n            data.writeUInt16LE(_extraLen, Constants.CENEXT);\n            // file comment length\n            data.writeUInt16LE(_comLen, Constants.CENCOM);\n            // volume number start\n            data.writeUInt16LE(_diskStart, Constants.CENDSK);\n            // internal file attributes\n            data.writeUInt16LE(_inattr, Constants.CENATT);\n            // external file attributes\n            data.writeUInt32LE(_attr, Constants.CENATX);\n            // LOC header offset\n            data.writeUInt32LE(_offset, Constants.CENOFF);\n            return data;\n        },\n\n        toJSON: function () {\n            const bytes = function (nr) {\n                return nr + \" bytes\";\n            };\n\n            return {\n                made: _verMade,\n                version: _version,\n                flags: _flags,\n                method: Utils.methodToString(_method),\n                time: this.time,\n                crc: \"0x\" + _crc.toString(16).toUpperCase(),\n                compressedSize: bytes(_compressedSize),\n                size: bytes(_size),\n                fileNameLength: bytes(_fnameLen),\n                extraLength: bytes(_extraLen),\n                commentLength: bytes(_comLen),\n                diskNumStart: _diskStart,\n                inAttr: _inattr,\n                attr: _attr,\n                offset: _offset,\n                centralHeaderSize: bytes(Constants.CENHDR + _fnameLen + _extraLen + _comLen)\n            };\n        },\n\n        toString: function () {\n            return JSON.stringify(this.toJSON(), null, \"\\t\");\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/headers/entryHeader.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/headers/index.js":
/*!***************************************************!*\
  !*** ../../node_modules/adm-zip/headers/index.js ***!
  \***************************************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

eval("exports.EntryHeader = __webpack_require__(/*! ./entryHeader */ \"../../node_modules/adm-zip/headers/entryHeader.js\");\nexports.MainHeader = __webpack_require__(/*! ./mainHeader */ \"../../node_modules/adm-zip/headers/mainHeader.js\");\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/headers/index.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/headers/mainHeader.js":
/*!********************************************************!*\
  !*** ../../node_modules/adm-zip/headers/mainHeader.js ***!
  \********************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("var Utils = __webpack_require__(/*! ../util */ \"../../node_modules/adm-zip/util/index.js\"),\n    Constants = Utils.Constants;\n\n/* The entries in the end of central directory */\nmodule.exports = function () {\n    var _volumeEntries = 0,\n        _totalEntries = 0,\n        _size = 0,\n        _offset = 0,\n        _commentLength = 0;\n\n    return {\n        get diskEntries() {\n            return _volumeEntries;\n        },\n        set diskEntries(/*Number*/ val) {\n            _volumeEntries = _totalEntries = val;\n        },\n\n        get totalEntries() {\n            return _totalEntries;\n        },\n        set totalEntries(/*Number*/ val) {\n            _totalEntries = _volumeEntries = val;\n        },\n\n        get size() {\n            return _size;\n        },\n        set size(/*Number*/ val) {\n            _size = val;\n        },\n\n        get offset() {\n            return _offset;\n        },\n        set offset(/*Number*/ val) {\n            _offset = val;\n        },\n\n        get commentLength() {\n            return _commentLength;\n        },\n        set commentLength(/*Number*/ val) {\n            _commentLength = val;\n        },\n\n        get mainHeaderSize() {\n            return Constants.ENDHDR + _commentLength;\n        },\n\n        loadFromBinary: function (/*Buffer*/ data) {\n            // data should be 22 bytes and start with \"PK 05 06\"\n            // or be 56+ bytes and start with \"PK 06 06\" for Zip64\n            if (\n                (data.length !== Constants.ENDHDR || data.readUInt32LE(0) !== Constants.ENDSIG) &&\n                (data.length < Constants.ZIP64HDR || data.readUInt32LE(0) !== Constants.ZIP64SIG)\n            ) {\n                throw Utils.Errors.INVALID_END();\n            }\n\n            if (data.readUInt32LE(0) === Constants.ENDSIG) {\n                // number of entries on this volume\n                _volumeEntries = data.readUInt16LE(Constants.ENDSUB);\n                // total number of entries\n                _totalEntries = data.readUInt16LE(Constants.ENDTOT);\n                // central directory size in bytes\n                _size = data.readUInt32LE(Constants.ENDSIZ);\n                // offset of first CEN header\n                _offset = data.readUInt32LE(Constants.ENDOFF);\n                // zip file comment length\n                _commentLength = data.readUInt16LE(Constants.ENDCOM);\n            } else {\n                // number of entries on this volume\n                _volumeEntries = Utils.readBigUInt64LE(data, Constants.ZIP64SUB);\n                // total number of entries\n                _totalEntries = Utils.readBigUInt64LE(data, Constants.ZIP64TOT);\n                // central directory size in bytes\n                _size = Utils.readBigUInt64LE(data, Constants.ZIP64SIZE);\n                // offset of first CEN header\n                _offset = Utils.readBigUInt64LE(data, Constants.ZIP64OFF);\n\n                _commentLength = 0;\n            }\n        },\n\n        toBinary: function () {\n            var b = Buffer.alloc(Constants.ENDHDR + _commentLength);\n            // \"PK 05 06\" signature\n            b.writeUInt32LE(Constants.ENDSIG, 0);\n            b.writeUInt32LE(0, 4);\n            // number of entries on this volume\n            b.writeUInt16LE(_volumeEntries, Constants.ENDSUB);\n            // total number of entries\n            b.writeUInt16LE(_totalEntries, Constants.ENDTOT);\n            // central directory size in bytes\n            b.writeUInt32LE(_size, Constants.ENDSIZ);\n            // offset of first CEN header\n            b.writeUInt32LE(_offset, Constants.ENDOFF);\n            // zip file comment length\n            b.writeUInt16LE(_commentLength, Constants.ENDCOM);\n            // fill comment memory with spaces so no garbage is left there\n            b.fill(\" \", Constants.ENDHDR);\n\n            return b;\n        },\n\n        toJSON: function () {\n            // creates 0x0000 style output\n            const offset = function (nr, len) {\n                let offs = nr.toString(16).toUpperCase();\n                while (offs.length < len) offs = \"0\" + offs;\n                return \"0x\" + offs;\n            };\n\n            return {\n                diskEntries: _volumeEntries,\n                totalEntries: _totalEntries,\n                size: _size + \" bytes\",\n                offset: offset(_offset, 4),\n                commentLength: _commentLength\n            };\n        },\n\n        toString: function () {\n            return JSON.stringify(this.toJSON(), null, \"\\t\");\n        }\n    };\n};\n// Misspelled\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/headers/mainHeader.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/methods/deflater.js":
/*!******************************************************!*\
  !*** ../../node_modules/adm-zip/methods/deflater.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("module.exports = function (/*Buffer*/ inbuf) {\n    var zlib = __webpack_require__(/*! zlib */ \"zlib\");\n\n    var opts = { chunkSize: (parseInt(inbuf.length / 1024) + 1) * 1024 };\n\n    return {\n        deflate: function () {\n            return zlib.deflateRawSync(inbuf, opts);\n        },\n\n        deflateAsync: function (/*Function*/ callback) {\n            var tmp = zlib.createDeflateRaw(opts),\n                parts = [],\n                total = 0;\n            tmp.on(\"data\", function (data) {\n                parts.push(data);\n                total += data.length;\n            });\n            tmp.on(\"end\", function () {\n                var buf = Buffer.alloc(total),\n                    written = 0;\n                buf.fill(0);\n                for (var i = 0; i < parts.length; i++) {\n                    var part = parts[i];\n                    part.copy(buf, written);\n                    written += part.length;\n                }\n                callback && callback(buf);\n            });\n            tmp.end(inbuf);\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/methods/deflater.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/methods/index.js":
/*!***************************************************!*\
  !*** ../../node_modules/adm-zip/methods/index.js ***!
  \***************************************************/
/***/ ((__unused_webpack_module, exports, __webpack_require__) => {

eval("exports.Deflater = __webpack_require__(/*! ./deflater */ \"../../node_modules/adm-zip/methods/deflater.js\");\nexports.Inflater = __webpack_require__(/*! ./inflater */ \"../../node_modules/adm-zip/methods/inflater.js\");\nexports.ZipCrypto = __webpack_require__(/*! ./zipcrypto */ \"../../node_modules/adm-zip/methods/zipcrypto.js\");\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/methods/index.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/methods/inflater.js":
/*!******************************************************!*\
  !*** ../../node_modules/adm-zip/methods/inflater.js ***!
  \******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("const version = +(process.versions ? process.versions.node : \"\").split(\".\")[0] || 0;\n\nmodule.exports = function (/*Buffer*/ inbuf, /*number*/ expectedLength) {\n    var zlib = __webpack_require__(/*! zlib */ \"zlib\");\n    const option = version >= 15 && expectedLength > 0 ? { maxOutputLength: expectedLength } : {};\n\n    return {\n        inflate: function () {\n            return zlib.inflateRawSync(inbuf, option);\n        },\n\n        inflateAsync: function (/*Function*/ callback) {\n            var tmp = zlib.createInflateRaw(option),\n                parts = [],\n                total = 0;\n            tmp.on(\"data\", function (data) {\n                parts.push(data);\n                total += data.length;\n            });\n            tmp.on(\"end\", function () {\n                var buf = Buffer.alloc(total),\n                    written = 0;\n                buf.fill(0);\n                for (var i = 0; i < parts.length; i++) {\n                    var part = parts[i];\n                    part.copy(buf, written);\n                    written += part.length;\n                }\n                callback && callback(buf);\n            });\n            tmp.end(inbuf);\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/methods/inflater.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/methods/zipcrypto.js":
/*!*******************************************************!*\
  !*** ../../node_modules/adm-zip/methods/zipcrypto.js ***!
  \*******************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

"use strict";
eval("\n\n// node crypt, we use it for generate salt\n// eslint-disable-next-line node/no-unsupported-features/node-builtins\nconst { randomFillSync } = __webpack_require__(/*! crypto */ \"crypto\");\nconst Errors = __webpack_require__(/*! ../util/errors */ \"../../node_modules/adm-zip/util/errors.js\");\n\n// generate CRC32 lookup table\nconst crctable = new Uint32Array(256).map((t, crc) => {\n    for (let j = 0; j < 8; j++) {\n        if (0 !== (crc & 1)) {\n            crc = (crc >>> 1) ^ 0xedb88320;\n        } else {\n            crc >>>= 1;\n        }\n    }\n    return crc >>> 0;\n});\n\n// C-style uInt32 Multiply (discards higher bits, when JS multiply discards lower bits)\nconst uMul = (a, b) => Math.imul(a, b) >>> 0;\n\n// crc32 byte single update (actually same function is part of utils.crc32 function :) )\nconst crc32update = (pCrc32, bval) => {\n    return crctable[(pCrc32 ^ bval) & 0xff] ^ (pCrc32 >>> 8);\n};\n\n// function for generating salt for encrytion header\nconst genSalt = () => {\n    if (\"function\" === typeof randomFillSync) {\n        return randomFillSync(Buffer.alloc(12));\n    } else {\n        // fallback if function is not defined\n        return genSalt.node();\n    }\n};\n\n// salt generation with node random function (mainly as fallback)\ngenSalt.node = () => {\n    const salt = Buffer.alloc(12);\n    const len = salt.length;\n    for (let i = 0; i < len; i++) salt[i] = (Math.random() * 256) & 0xff;\n    return salt;\n};\n\n// general config\nconst config = {\n    genSalt\n};\n\n// Class Initkeys handles same basic ops with keys\nfunction Initkeys(pw) {\n    const pass = Buffer.isBuffer(pw) ? pw : Buffer.from(pw);\n    this.keys = new Uint32Array([0x12345678, 0x23456789, 0x34567890]);\n    for (let i = 0; i < pass.length; i++) {\n        this.updateKeys(pass[i]);\n    }\n}\n\nInitkeys.prototype.updateKeys = function (byteValue) {\n    const keys = this.keys;\n    keys[0] = crc32update(keys[0], byteValue);\n    keys[1] += keys[0] & 0xff;\n    keys[1] = uMul(keys[1], 134775813) + 1;\n    keys[2] = crc32update(keys[2], keys[1] >>> 24);\n    return byteValue;\n};\n\nInitkeys.prototype.next = function () {\n    const k = (this.keys[2] | 2) >>> 0; // key\n    return (uMul(k, k ^ 1) >> 8) & 0xff; // decode\n};\n\nfunction make_decrypter(/*Buffer*/ pwd) {\n    // 1. Stage initialize key\n    const keys = new Initkeys(pwd);\n\n    // return decrypter function\n    return function (/*Buffer*/ data) {\n        // result - we create new Buffer for results\n        const result = Buffer.alloc(data.length);\n        let pos = 0;\n        // process input data\n        for (let c of data) {\n            //c ^= keys.next();\n            //result[pos++] = c; // decode & Save Value\n            result[pos++] = keys.updateKeys(c ^ keys.next()); // update keys with decoded byte\n        }\n        return result;\n    };\n}\n\nfunction make_encrypter(/*Buffer*/ pwd) {\n    // 1. Stage initialize key\n    const keys = new Initkeys(pwd);\n\n    // return encrypting function, result and pos is here so we dont have to merge buffers later\n    return function (/*Buffer*/ data, /*Buffer*/ result, /* Number */ pos = 0) {\n        // result - we create new Buffer for results\n        if (!result) result = Buffer.alloc(data.length);\n        // process input data\n        for (let c of data) {\n            const k = keys.next(); // save key byte\n            result[pos++] = c ^ k; // save val\n            keys.updateKeys(c); // update keys with decoded byte\n        }\n        return result;\n    };\n}\n\nfunction decrypt(/*Buffer*/ data, /*Object*/ header, /*String, Buffer*/ pwd) {\n    if (!data || !Buffer.isBuffer(data) || data.length < 12) {\n        return Buffer.alloc(0);\n    }\n\n    // 1. We Initialize and generate decrypting function\n    const decrypter = make_decrypter(pwd);\n\n    // 2. decrypt salt what is always 12 bytes and is a part of file content\n    const salt = decrypter(data.slice(0, 12));\n\n    // if bit 3 (0x08) of the general-purpose flags field is set, check salt[11] with the high byte of the header time\n    // 2 byte data block (as per Info-Zip spec), otherwise check with the high byte of the header entry\n    const verifyByte = (header.flags & 0x8) === 0x8 ? header.timeHighByte : header.crc >>> 24;\n\n    //3. does password meet expectations\n    if (salt[11] !== verifyByte) {\n        throw Errors.WRONG_PASSWORD();\n    }\n\n    // 4. decode content\n    return decrypter(data.slice(12));\n}\n\n// lets add way to populate salt, NOT RECOMMENDED for production but maybe useful for testing general functionality\nfunction _salter(data) {\n    if (Buffer.isBuffer(data) && data.length >= 12) {\n        // be aware - currently salting buffer data is modified\n        config.genSalt = function () {\n            return data.slice(0, 12);\n        };\n    } else if (data === \"node\") {\n        // test salt generation with node random function\n        config.genSalt = genSalt.node;\n    } else {\n        // if value is not acceptable config gets reset.\n        config.genSalt = genSalt;\n    }\n}\n\nfunction encrypt(/*Buffer*/ data, /*Object*/ header, /*String, Buffer*/ pwd, /*Boolean*/ oldlike = false) {\n    // 1. test data if data is not Buffer we make buffer from it\n    if (data == null) data = Buffer.alloc(0);\n    // if data is not buffer be make buffer from it\n    if (!Buffer.isBuffer(data)) data = Buffer.from(data.toString());\n\n    // 2. We Initialize and generate encrypting function\n    const encrypter = make_encrypter(pwd);\n\n    // 3. generate salt (12-bytes of random data)\n    const salt = config.genSalt();\n    salt[11] = (header.crc >>> 24) & 0xff;\n\n    // old implementations (before PKZip 2.04g) used two byte check\n    if (oldlike) salt[10] = (header.crc >>> 16) & 0xff;\n\n    // 4. create output\n    const result = Buffer.alloc(data.length + 12);\n    encrypter(salt, result);\n\n    // finally encode content\n    return encrypter(data, result, 12);\n}\n\nmodule.exports = { decrypt, encrypt, _salter };\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/methods/zipcrypto.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/constants.js":
/*!****************************************************!*\
  !*** ../../node_modules/adm-zip/util/constants.js ***!
  \****************************************************/
/***/ ((module) => {

eval("module.exports = {\n    /* The local file header */\n    LOCHDR           : 30, // LOC header size\n    LOCSIG           : 0x04034b50, // \"PK\\003\\004\"\n    LOCVER           : 4,\t// version needed to extract\n    LOCFLG           : 6, // general purpose bit flag\n    LOCHOW           : 8, // compression method\n    LOCTIM           : 10, // modification time (2 bytes time, 2 bytes date)\n    LOCCRC           : 14, // uncompressed file crc-32 value\n    LOCSIZ           : 18, // compressed size\n    LOCLEN           : 22, // uncompressed size\n    LOCNAM           : 26, // filename length\n    LOCEXT           : 28, // extra field length\n\n    /* The Data descriptor */\n    EXTSIG           : 0x08074b50, // \"PK\\007\\008\"\n    EXTHDR           : 16, // EXT header size\n    EXTCRC           : 4, // uncompressed file crc-32 value\n    EXTSIZ           : 8, // compressed size\n    EXTLEN           : 12, // uncompressed size\n\n    /* The central directory file header */\n    CENHDR           : 46, // CEN header size\n    CENSIG           : 0x02014b50, // \"PK\\001\\002\"\n    CENVEM           : 4, // version made by\n    CENVER           : 6, // version needed to extract\n    CENFLG           : 8, // encrypt, decrypt flags\n    CENHOW           : 10, // compression method\n    CENTIM           : 12, // modification time (2 bytes time, 2 bytes date)\n    CENCRC           : 16, // uncompressed file crc-32 value\n    CENSIZ           : 20, // compressed size\n    CENLEN           : 24, // uncompressed size\n    CENNAM           : 28, // filename length\n    CENEXT           : 30, // extra field length\n    CENCOM           : 32, // file comment length\n    CENDSK           : 34, // volume number start\n    CENATT           : 36, // internal file attributes\n    CENATX           : 38, // external file attributes (host system dependent)\n    CENOFF           : 42, // LOC header offset\n\n    /* The entries in the end of central directory */\n    ENDHDR           : 22, // END header size\n    ENDSIG           : 0x06054b50, // \"PK\\005\\006\"\n    ENDSUB           : 8, // number of entries on this disk\n    ENDTOT           : 10, // total number of entries\n    ENDSIZ           : 12, // central directory size in bytes\n    ENDOFF           : 16, // offset of first CEN header\n    ENDCOM           : 20, // zip file comment length\n\n    END64HDR         : 20, // zip64 END header size\n    END64SIG         : 0x07064b50, // zip64 Locator signature, \"PK\\006\\007\"\n    END64START       : 4, // number of the disk with the start of the zip64\n    END64OFF         : 8, // relative offset of the zip64 end of central directory\n    END64NUMDISKS    : 16, // total number of disks\n\n    ZIP64SIG         : 0x06064b50, // zip64 signature, \"PK\\006\\006\"\n    ZIP64HDR         : 56, // zip64 record minimum size\n    ZIP64LEAD        : 12, // leading bytes at the start of the record, not counted by the value stored in ZIP64SIZE\n    ZIP64SIZE        : 4, // zip64 size of the central directory record\n    ZIP64VEM         : 12, // zip64 version made by\n    ZIP64VER         : 14, // zip64 version needed to extract\n    ZIP64DSK         : 16, // zip64 number of this disk\n    ZIP64DSKDIR      : 20, // number of the disk with the start of the record directory\n    ZIP64SUB         : 24, // number of entries on this disk\n    ZIP64TOT         : 32, // total number of entries\n    ZIP64SIZB        : 40, // zip64 central directory size in bytes\n    ZIP64OFF         : 48, // offset of start of central directory with respect to the starting disk number\n    ZIP64EXTRA       : 56, // extensible data sector\n\n    /* Compression methods */\n    STORED           : 0, // no compression\n    SHRUNK           : 1, // shrunk\n    REDUCED1         : 2, // reduced with compression factor 1\n    REDUCED2         : 3, // reduced with compression factor 2\n    REDUCED3         : 4, // reduced with compression factor 3\n    REDUCED4         : 5, // reduced with compression factor 4\n    IMPLODED         : 6, // imploded\n    // 7 reserved for Tokenizing compression algorithm\n    DEFLATED         : 8, // deflated\n    ENHANCED_DEFLATED: 9, // enhanced deflated\n    PKWARE           : 10,// PKWare DCL imploded\n    // 11 reserved by PKWARE\n    BZIP2            : 12, //  compressed using BZIP2\n    // 13 reserved by PKWARE\n    LZMA             : 14, // LZMA\n    // 15-17 reserved by PKWARE\n    IBM_TERSE        : 18, // compressed using IBM TERSE\n    IBM_LZ77         : 19, // IBM LZ77 z\n    AES_ENCRYPT      : 99, // WinZIP AES encryption method\n\n    /* General purpose bit flag */\n    // values can obtained with expression 2**bitnr\n    FLG_ENC          : 1,    // Bit 0: encrypted file\n    FLG_COMP1        : 2,    // Bit 1, compression option\n    FLG_COMP2        : 4,    // Bit 2, compression option\n    FLG_DESC         : 8,    // Bit 3, data descriptor\n    FLG_ENH          : 16,   // Bit 4, enhanced deflating\n    FLG_PATCH        : 32,   // Bit 5, indicates that the file is compressed patched data.\n    FLG_STR          : 64,   // Bit 6, strong encryption (patented)\n                             // Bits 7-10: Currently unused.\n    FLG_EFS          : 2048, // Bit 11: Language encoding flag (EFS)\n                             // Bit 12: Reserved by PKWARE for enhanced compression.\n                             // Bit 13: encrypted the Central Directory (patented).\n                             // Bits 14-15: Reserved by PKWARE.\n    FLG_MSK          : 4096, // mask header values\n\n    /* Load type */\n    FILE             : 2,\n    BUFFER           : 1,\n    NONE             : 0,\n\n    /* 4.5 Extensible data fields */\n    EF_ID            : 0,\n    EF_SIZE          : 2,\n\n    /* Header IDs */\n    ID_ZIP64         : 0x0001,\n    ID_AVINFO        : 0x0007,\n    ID_PFS           : 0x0008,\n    ID_OS2           : 0x0009,\n    ID_NTFS          : 0x000a,\n    ID_OPENVMS       : 0x000c,\n    ID_UNIX          : 0x000d,\n    ID_FORK          : 0x000e,\n    ID_PATCH         : 0x000f,\n    ID_X509_PKCS7    : 0x0014,\n    ID_X509_CERTID_F : 0x0015,\n    ID_X509_CERTID_C : 0x0016,\n    ID_STRONGENC     : 0x0017,\n    ID_RECORD_MGT    : 0x0018,\n    ID_X509_PKCS7_RL : 0x0019,\n    ID_IBM1          : 0x0065,\n    ID_IBM2          : 0x0066,\n    ID_POSZIP        : 0x4690,\n\n    EF_ZIP64_OR_32   : 0xffffffff,\n    EF_ZIP64_OR_16   : 0xffff,\n    EF_ZIP64_SUNCOMP : 0,\n    EF_ZIP64_SCOMP   : 8,\n    EF_ZIP64_RHO     : 16,\n    EF_ZIP64_DSN     : 24\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/constants.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/decoder.js":
/*!**************************************************!*\
  !*** ../../node_modules/adm-zip/util/decoder.js ***!
  \**************************************************/
/***/ ((module) => {

eval("module.exports = {\n    efs: true,\n    encode: (data) => Buffer.from(data, \"utf8\"),\n    decode: (data) => data.toString(\"utf8\")\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/decoder.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/errors.js":
/*!*************************************************!*\
  !*** ../../node_modules/adm-zip/util/errors.js ***!
  \*************************************************/
/***/ ((__unused_webpack_module, exports) => {

eval("const errors = {\n    /* Header error messages */\n    INVALID_LOC: \"Invalid LOC header (bad signature)\",\n    INVALID_CEN: \"Invalid CEN header (bad signature)\",\n    INVALID_END: \"Invalid END header (bad signature)\",\n\n    /* Descriptor */\n    DESCRIPTOR_NOT_EXIST: \"No descriptor present\",\n    DESCRIPTOR_UNKNOWN: \"Unknown descriptor format\",\n    DESCRIPTOR_FAULTY: \"Descriptor data is malformed\",\n\n    /* ZipEntry error messages*/\n    NO_DATA: \"Nothing to decompress\",\n    BAD_CRC: \"CRC32 checksum failed {0}\",\n    FILE_IN_THE_WAY: \"There is a file in the way: {0}\",\n    UNKNOWN_METHOD: \"Invalid/unsupported compression method\",\n\n    /* Inflater error messages */\n    AVAIL_DATA: \"inflate::Available inflate data did not terminate\",\n    INVALID_DISTANCE: \"inflate::Invalid literal/length or distance code in fixed or dynamic block\",\n    TO_MANY_CODES: \"inflate::Dynamic block code description: too many length or distance codes\",\n    INVALID_REPEAT_LEN: \"inflate::Dynamic block code description: repeat more than specified lengths\",\n    INVALID_REPEAT_FIRST: \"inflate::Dynamic block code description: repeat lengths with no first length\",\n    INCOMPLETE_CODES: \"inflate::Dynamic block code description: code lengths codes incomplete\",\n    INVALID_DYN_DISTANCE: \"inflate::Dynamic block code description: invalid distance code lengths\",\n    INVALID_CODES_LEN: \"inflate::Dynamic block code description: invalid literal/length code lengths\",\n    INVALID_STORE_BLOCK: \"inflate::Stored block length did not match one's complement\",\n    INVALID_BLOCK_TYPE: \"inflate::Invalid block type (type == 3)\",\n\n    /* ADM-ZIP error messages */\n    CANT_EXTRACT_FILE: \"Could not extract the file\",\n    CANT_OVERRIDE: \"Target file already exists\",\n    DISK_ENTRY_TOO_LARGE: \"Number of disk entries is too large\",\n    NO_ZIP: \"No zip file was loaded\",\n    NO_ENTRY: \"Entry doesn't exist\",\n    DIRECTORY_CONTENT_ERROR: \"A directory cannot have content\",\n    FILE_NOT_FOUND: 'File not found: \"{0}\"',\n    NOT_IMPLEMENTED: \"Not implemented\",\n    INVALID_FILENAME: \"Invalid filename\",\n    INVALID_FORMAT: \"Invalid or unsupported zip format. No END header found\",\n    INVALID_PASS_PARAM: \"Incompatible password parameter\",\n    WRONG_PASSWORD: \"Wrong Password\",\n\n    /* ADM-ZIP */\n    COMMENT_TOO_LONG: \"Comment is too long\", // Comment can be max 65535 bytes long (NOTE: some non-US characters may take more space)\n    EXTRA_FIELD_PARSE_ERROR: \"Extra field parsing error\"\n};\n\n// template\nfunction E(message) {\n    return function (...args) {\n        if (args.length) { // Allow {0} .. {9} arguments in error message, based on argument number\n            message = message.replace(/\\{(\\d)\\}/g, (_, n) => args[n] || '');\n        }\n\n        return new Error('ADM-ZIP: ' + message);\n    };\n}\n\n// Init errors with template\nfor (const msg of Object.keys(errors)) {\n    exports[msg] = E(errors[msg]);\n}\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/errors.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/fattr.js":
/*!************************************************!*\
  !*** ../../node_modules/adm-zip/util/fattr.js ***!
  \************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("const pth = __webpack_require__(/*! path */ \"path\");\n\nmodule.exports = function (/*String*/ path, /*Utils object*/ { fs }) {\n    var _path = path || \"\",\n        _obj = newAttr(),\n        _stat = null;\n\n    function newAttr() {\n        return {\n            directory: false,\n            readonly: false,\n            hidden: false,\n            executable: false,\n            mtime: 0,\n            atime: 0\n        };\n    }\n\n    if (_path && fs.existsSync(_path)) {\n        _stat = fs.statSync(_path);\n        _obj.directory = _stat.isDirectory();\n        _obj.mtime = _stat.mtime;\n        _obj.atime = _stat.atime;\n        _obj.executable = (0o111 & _stat.mode) !== 0; // file is executable who ever har right not just owner\n        _obj.readonly = (0o200 & _stat.mode) === 0; // readonly if owner has no write right\n        _obj.hidden = pth.basename(_path)[0] === \".\";\n    } else {\n        console.warn(\"Invalid path: \" + _path);\n    }\n\n    return {\n        get directory() {\n            return _obj.directory;\n        },\n\n        get readOnly() {\n            return _obj.readonly;\n        },\n\n        get hidden() {\n            return _obj.hidden;\n        },\n\n        get mtime() {\n            return _obj.mtime;\n        },\n\n        get atime() {\n            return _obj.atime;\n        },\n\n        get executable() {\n            return _obj.executable;\n        },\n\n        decodeAttributes: function () {},\n\n        encodeAttributes: function () {},\n\n        toJSON: function () {\n            return {\n                path: _path,\n                isDirectory: _obj.directory,\n                isReadOnly: _obj.readonly,\n                isHidden: _obj.hidden,\n                isExecutable: _obj.executable,\n                mTime: _obj.mtime,\n                aTime: _obj.atime\n            };\n        },\n\n        toString: function () {\n            return JSON.stringify(this.toJSON(), null, \"\\t\");\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/fattr.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/index.js":
/*!************************************************!*\
  !*** ../../node_modules/adm-zip/util/index.js ***!
  \************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("module.exports = __webpack_require__(/*! ./utils */ \"../../node_modules/adm-zip/util/utils.js\");\nmodule.exports.Constants = __webpack_require__(/*! ./constants */ \"../../node_modules/adm-zip/util/constants.js\");\nmodule.exports.Errors = __webpack_require__(/*! ./errors */ \"../../node_modules/adm-zip/util/errors.js\");\nmodule.exports.FileAttr = __webpack_require__(/*! ./fattr */ \"../../node_modules/adm-zip/util/fattr.js\");\nmodule.exports.decoder = __webpack_require__(/*! ./decoder */ \"../../node_modules/adm-zip/util/decoder.js\");\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/index.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/util/utils.js":
/*!************************************************!*\
  !*** ../../node_modules/adm-zip/util/utils.js ***!
  \************************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("const fsystem = __webpack_require__(/*! fs */ \"fs\");\nconst pth = __webpack_require__(/*! path */ \"path\");\nconst Constants = __webpack_require__(/*! ./constants */ \"../../node_modules/adm-zip/util/constants.js\");\nconst Errors = __webpack_require__(/*! ./errors */ \"../../node_modules/adm-zip/util/errors.js\");\nconst isWin = typeof process === \"object\" && \"win32\" === process.platform;\n\nconst is_Obj = (obj) => typeof obj === \"object\" && obj !== null;\n\n// generate CRC32 lookup table\nconst crcTable = new Uint32Array(256).map((t, c) => {\n    for (let k = 0; k < 8; k++) {\n        if ((c & 1) !== 0) {\n            c = 0xedb88320 ^ (c >>> 1);\n        } else {\n            c >>>= 1;\n        }\n    }\n    return c >>> 0;\n});\n\n// UTILS functions\n\nfunction Utils(opts) {\n    this.sep = pth.sep;\n    this.fs = fsystem;\n\n    if (is_Obj(opts)) {\n        // custom filesystem\n        if (is_Obj(opts.fs) && typeof opts.fs.statSync === \"function\") {\n            this.fs = opts.fs;\n        }\n    }\n}\n\nmodule.exports = Utils;\n\n// INSTANTIABLE functions\n\nUtils.prototype.makeDir = function (/*String*/ folder) {\n    const self = this;\n\n    // Sync - make directories tree\n    function mkdirSync(/*String*/ fpath) {\n        let resolvedPath = fpath.split(self.sep)[0];\n        fpath.split(self.sep).forEach(function (name) {\n            if (!name || name.substr(-1, 1) === \":\") return;\n            resolvedPath += self.sep + name;\n            var stat;\n            try {\n                stat = self.fs.statSync(resolvedPath);\n            } catch (e) {\n                self.fs.mkdirSync(resolvedPath);\n            }\n            if (stat && stat.isFile()) throw Errors.FILE_IN_THE_WAY(`\"${resolvedPath}\"`);\n        });\n    }\n\n    mkdirSync(folder);\n};\n\nUtils.prototype.writeFileTo = function (/*String*/ path, /*Buffer*/ content, /*Boolean*/ overwrite, /*Number*/ attr) {\n    const self = this;\n    if (self.fs.existsSync(path)) {\n        if (!overwrite) return false; // cannot overwrite\n\n        var stat = self.fs.statSync(path);\n        if (stat.isDirectory()) {\n            return false;\n        }\n    }\n    var folder = pth.dirname(path);\n    if (!self.fs.existsSync(folder)) {\n        self.makeDir(folder);\n    }\n\n    var fd;\n    try {\n        fd = self.fs.openSync(path, \"w\", 0o666); // 0666\n    } catch (e) {\n        self.fs.chmodSync(path, 0o666);\n        fd = self.fs.openSync(path, \"w\", 0o666);\n    }\n    if (fd) {\n        try {\n            self.fs.writeSync(fd, content, 0, content.length, 0);\n        } finally {\n            self.fs.closeSync(fd);\n        }\n    }\n    self.fs.chmodSync(path, attr || 0o666);\n    return true;\n};\n\nUtils.prototype.writeFileToAsync = function (/*String*/ path, /*Buffer*/ content, /*Boolean*/ overwrite, /*Number*/ attr, /*Function*/ callback) {\n    if (typeof attr === \"function\") {\n        callback = attr;\n        attr = undefined;\n    }\n\n    const self = this;\n\n    self.fs.exists(path, function (exist) {\n        if (exist && !overwrite) return callback(false);\n\n        self.fs.stat(path, function (err, stat) {\n            if (exist && stat.isDirectory()) {\n                return callback(false);\n            }\n\n            var folder = pth.dirname(path);\n            self.fs.exists(folder, function (exists) {\n                if (!exists) self.makeDir(folder);\n\n                self.fs.open(path, \"w\", 0o666, function (err, fd) {\n                    if (err) {\n                        self.fs.chmod(path, 0o666, function () {\n                            self.fs.open(path, \"w\", 0o666, function (err, fd) {\n                                self.fs.write(fd, content, 0, content.length, 0, function () {\n                                    self.fs.close(fd, function () {\n                                        self.fs.chmod(path, attr || 0o666, function () {\n                                            callback(true);\n                                        });\n                                    });\n                                });\n                            });\n                        });\n                    } else if (fd) {\n                        self.fs.write(fd, content, 0, content.length, 0, function () {\n                            self.fs.close(fd, function () {\n                                self.fs.chmod(path, attr || 0o666, function () {\n                                    callback(true);\n                                });\n                            });\n                        });\n                    } else {\n                        self.fs.chmod(path, attr || 0o666, function () {\n                            callback(true);\n                        });\n                    }\n                });\n            });\n        });\n    });\n};\n\nUtils.prototype.findFiles = function (/*String*/ path) {\n    const self = this;\n\n    function findSync(/*String*/ dir, /*RegExp*/ pattern, /*Boolean*/ recursive) {\n        if (typeof pattern === \"boolean\") {\n            recursive = pattern;\n            pattern = undefined;\n        }\n        let files = [];\n        self.fs.readdirSync(dir).forEach(function (file) {\n            const path = pth.join(dir, file);\n            const stat = self.fs.statSync(path);\n\n            if (!pattern || pattern.test(path)) {\n                files.push(pth.normalize(path) + (stat.isDirectory() ? self.sep : \"\"));\n            }\n\n            if (stat.isDirectory() && recursive) files = files.concat(findSync(path, pattern, recursive));\n        });\n        return files;\n    }\n\n    return findSync(path, undefined, true);\n};\n\n/**\n * Callback for showing if everything was done.\n *\n * @callback filelistCallback\n * @param {Error} err - Error object\n * @param {string[]} list - was request fully completed\n */\n\n/**\n *\n * @param {string} dir\n * @param {filelistCallback} cb\n */\nUtils.prototype.findFilesAsync = function (dir, cb) {\n    const self = this;\n    let results = [];\n    self.fs.readdir(dir, function (err, list) {\n        if (err) return cb(err);\n        let list_length = list.length;\n        if (!list_length) return cb(null, results);\n        list.forEach(function (file) {\n            file = pth.join(dir, file);\n            self.fs.stat(file, function (err, stat) {\n                if (err) return cb(err);\n                if (stat) {\n                    results.push(pth.normalize(file) + (stat.isDirectory() ? self.sep : \"\"));\n                    if (stat.isDirectory()) {\n                        self.findFilesAsync(file, function (err, res) {\n                            if (err) return cb(err);\n                            results = results.concat(res);\n                            if (!--list_length) cb(null, results);\n                        });\n                    } else {\n                        if (!--list_length) cb(null, results);\n                    }\n                }\n            });\n        });\n    });\n};\n\nUtils.prototype.getAttributes = function () {};\n\nUtils.prototype.setAttributes = function () {};\n\n// STATIC functions\n\n// crc32 single update (it is part of crc32)\nUtils.crc32update = function (crc, byte) {\n    return crcTable[(crc ^ byte) & 0xff] ^ (crc >>> 8);\n};\n\nUtils.crc32 = function (buf) {\n    if (typeof buf === \"string\") {\n        buf = Buffer.from(buf, \"utf8\");\n    }\n\n    let len = buf.length;\n    let crc = ~0;\n    for (let off = 0; off < len; ) crc = Utils.crc32update(crc, buf[off++]);\n    // xor and cast as uint32 number\n    return ~crc >>> 0;\n};\n\nUtils.methodToString = function (/*Number*/ method) {\n    switch (method) {\n        case Constants.STORED:\n            return \"STORED (\" + method + \")\";\n        case Constants.DEFLATED:\n            return \"DEFLATED (\" + method + \")\";\n        default:\n            return \"UNSUPPORTED (\" + method + \")\";\n    }\n};\n\n/**\n * removes \"..\" style path elements\n * @param {string} path - fixable path\n * @returns string - fixed filepath\n */\nUtils.canonical = function (/*string*/ path) {\n    if (!path) return \"\";\n    // trick normalize think path is absolute\n    const safeSuffix = pth.posix.normalize(\"/\" + path.split(\"\\\\\").join(\"/\"));\n    return pth.join(\".\", safeSuffix);\n};\n\n/**\n * fix file names in achive\n * @param {string} path - fixable path\n * @returns string - fixed filepath\n */\n\nUtils.zipnamefix = function (path) {\n    if (!path) return \"\";\n    // trick normalize think path is absolute\n    const safeSuffix = pth.posix.normalize(\"/\" + path.split(\"\\\\\").join(\"/\"));\n    return pth.posix.join(\".\", safeSuffix);\n};\n\n/**\n *\n * @param {Array} arr\n * @param {function} callback\n * @returns\n */\nUtils.findLast = function (arr, callback) {\n    if (!Array.isArray(arr)) throw new TypeError(\"arr is not array\");\n\n    const len = arr.length >>> 0;\n    for (let i = len - 1; i >= 0; i--) {\n        if (callback(arr[i], i, arr)) {\n            return arr[i];\n        }\n    }\n    return void 0;\n};\n\n// make abolute paths taking prefix as root folder\nUtils.sanitize = function (/*string*/ prefix, /*string*/ name) {\n    prefix = pth.resolve(pth.normalize(prefix));\n    var parts = name.split(\"/\");\n    for (var i = 0, l = parts.length; i < l; i++) {\n        var path = pth.normalize(pth.join(prefix, parts.slice(i, l).join(pth.sep)));\n        if (path.indexOf(prefix) === 0) {\n            return path;\n        }\n    }\n    return pth.normalize(pth.join(prefix, pth.basename(name)));\n};\n\n// converts buffer, Uint8Array, string types to buffer\nUtils.toBuffer = function toBuffer(/*buffer, Uint8Array, string*/ input, /* function */ encoder) {\n    if (Buffer.isBuffer(input)) {\n        return input;\n    } else if (input instanceof Uint8Array) {\n        return Buffer.from(input);\n    } else {\n        // expect string all other values are invalid and return empty buffer\n        return typeof input === \"string\" ? encoder(input) : Buffer.alloc(0);\n    }\n};\n\nUtils.readBigUInt64LE = function (/*Buffer*/ buffer, /*int*/ index) {\n    var slice = Buffer.from(buffer.slice(index, index + 8));\n    slice.swap64();\n\n    return parseInt(`0x${slice.toString(\"hex\")}`);\n};\n\nUtils.fromDOS2Date = function (val) {\n    return new Date(((val >> 25) & 0x7f) + 1980, Math.max(((val >> 21) & 0x0f) - 1, 0), Math.max((val >> 16) & 0x1f, 1), (val >> 11) & 0x1f, (val >> 5) & 0x3f, (val & 0x1f) << 1);\n};\n\nUtils.fromDate2DOS = function (val) {\n    let date = 0;\n    let time = 0;\n    if (val.getFullYear() > 1979) {\n        date = (((val.getFullYear() - 1980) & 0x7f) << 9) | ((val.getMonth() + 1) << 5) | val.getDate();\n        time = (val.getHours() << 11) | (val.getMinutes() << 5) | (val.getSeconds() >> 1);\n    }\n    return (date << 16) | time;\n};\n\nUtils.isWin = isWin; // Do we have windows system\nUtils.crcTable = crcTable;\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/util/utils.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/zipEntry.js":
/*!**********************************************!*\
  !*** ../../node_modules/adm-zip/zipEntry.js ***!
  \**********************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("var Utils = __webpack_require__(/*! ./util */ \"../../node_modules/adm-zip/util/index.js\"),\n    Headers = __webpack_require__(/*! ./headers */ \"../../node_modules/adm-zip/headers/index.js\"),\n    Constants = Utils.Constants,\n    Methods = __webpack_require__(/*! ./methods */ \"../../node_modules/adm-zip/methods/index.js\");\n\nmodule.exports = function (/** object */ options, /*Buffer*/ input) {\n    var _centralHeader = new Headers.EntryHeader(),\n        _entryName = Buffer.alloc(0),\n        _comment = Buffer.alloc(0),\n        _isDirectory = false,\n        uncompressedData = null,\n        _extra = Buffer.alloc(0),\n        _extralocal = Buffer.alloc(0),\n        _efs = true;\n\n    // assign options\n    const opts = options;\n\n    const decoder = typeof opts.decoder === \"object\" ? opts.decoder : Utils.decoder;\n    _efs = decoder.hasOwnProperty(\"efs\") ? decoder.efs : false;\n\n    function getCompressedDataFromZip() {\n        //if (!input || !Buffer.isBuffer(input)) {\n        if (!input || !(input instanceof Uint8Array)) {\n            return Buffer.alloc(0);\n        }\n        _extralocal = _centralHeader.loadLocalHeaderFromBinary(input);\n        return input.slice(_centralHeader.realDataOffset, _centralHeader.realDataOffset + _centralHeader.compressedSize);\n    }\n\n    function crc32OK(data) {\n        // if bit 3 (0x08) of the general-purpose flags field is set, then the CRC-32 and file sizes are not known when the local header is written\n        if (!_centralHeader.flags_desc) {\n            if (Utils.crc32(data) !== _centralHeader.localHeader.crc) {\n                return false;\n            }\n        } else {\n            const descriptor = {};\n            const dataEndOffset = _centralHeader.realDataOffset + _centralHeader.compressedSize;\n            // no descriptor after compressed data, instead new local header\n            if (input.readUInt32LE(dataEndOffset) == Constants.LOCSIG || input.readUInt32LE(dataEndOffset) == Constants.CENSIG) {\n                throw Utils.Errors.DESCRIPTOR_NOT_EXIST();\n            }\n\n            // get decriptor data\n            if (input.readUInt32LE(dataEndOffset) == Constants.EXTSIG) {\n                // descriptor with signature\n                descriptor.crc = input.readUInt32LE(dataEndOffset + Constants.EXTCRC);\n                descriptor.compressedSize = input.readUInt32LE(dataEndOffset + Constants.EXTSIZ);\n                descriptor.size = input.readUInt32LE(dataEndOffset + Constants.EXTLEN);\n            } else if (input.readUInt16LE(dataEndOffset + 12) === 0x4b50) {\n                // descriptor without signature (we check is new header starting where we expect)\n                descriptor.crc = input.readUInt32LE(dataEndOffset + Constants.EXTCRC - 4);\n                descriptor.compressedSize = input.readUInt32LE(dataEndOffset + Constants.EXTSIZ - 4);\n                descriptor.size = input.readUInt32LE(dataEndOffset + Constants.EXTLEN - 4);\n            } else {\n                throw Utils.Errors.DESCRIPTOR_UNKNOWN();\n            }\n\n            // check data integrity\n            if (descriptor.compressedSize !== _centralHeader.compressedSize || descriptor.size !== _centralHeader.size || descriptor.crc !== _centralHeader.crc) {\n                throw Utils.Errors.DESCRIPTOR_FAULTY();\n            }\n            if (Utils.crc32(data) !== descriptor.crc) {\n                return false;\n            }\n\n            // @TODO: zip64 bit descriptor fields\n            // if bit 3 is set and any value in local header \"zip64 Extended information\" extra field are set 0 (place holder)\n            // then 64-bit descriptor format is used instead of 32-bit\n            // central header - \"zip64 Extended information\" extra field should store real values and not place holders\n        }\n        return true;\n    }\n\n    function decompress(/*Boolean*/ async, /*Function*/ callback, /*String, Buffer*/ pass) {\n        if (typeof callback === \"undefined\" && typeof async === \"string\") {\n            pass = async;\n            async = void 0;\n        }\n        if (_isDirectory) {\n            if (async && callback) {\n                callback(Buffer.alloc(0), Utils.Errors.DIRECTORY_CONTENT_ERROR()); //si added error.\n            }\n            return Buffer.alloc(0);\n        }\n\n        var compressedData = getCompressedDataFromZip();\n\n        if (compressedData.length === 0) {\n            // File is empty, nothing to decompress.\n            if (async && callback) callback(compressedData);\n            return compressedData;\n        }\n\n        if (_centralHeader.encrypted) {\n            if (\"string\" !== typeof pass && !Buffer.isBuffer(pass)) {\n                throw Utils.Errors.INVALID_PASS_PARAM();\n            }\n            compressedData = Methods.ZipCrypto.decrypt(compressedData, _centralHeader, pass);\n        }\n\n        var data = Buffer.alloc(_centralHeader.size);\n\n        switch (_centralHeader.method) {\n            case Utils.Constants.STORED:\n                compressedData.copy(data);\n                if (!crc32OK(data)) {\n                    if (async && callback) callback(data, Utils.Errors.BAD_CRC()); //si added error\n                    throw Utils.Errors.BAD_CRC();\n                } else {\n                    //si added otherwise did not seem to return data.\n                    if (async && callback) callback(data);\n                    return data;\n                }\n            case Utils.Constants.DEFLATED:\n                var inflater = new Methods.Inflater(compressedData, _centralHeader.size);\n                if (!async) {\n                    const result = inflater.inflate(data);\n                    result.copy(data, 0);\n                    if (!crc32OK(data)) {\n                        throw Utils.Errors.BAD_CRC(`\"${decoder.decode(_entryName)}\"`);\n                    }\n                    return data;\n                } else {\n                    inflater.inflateAsync(function (result) {\n                        result.copy(result, 0);\n                        if (callback) {\n                            if (!crc32OK(result)) {\n                                callback(result, Utils.Errors.BAD_CRC()); //si added error\n                            } else {\n                                callback(result);\n                            }\n                        }\n                    });\n                }\n                break;\n            default:\n                if (async && callback) callback(Buffer.alloc(0), Utils.Errors.UNKNOWN_METHOD());\n                throw Utils.Errors.UNKNOWN_METHOD();\n        }\n    }\n\n    function compress(/*Boolean*/ async, /*Function*/ callback) {\n        if ((!uncompressedData || !uncompressedData.length) && Buffer.isBuffer(input)) {\n            // no data set or the data wasn't changed to require recompression\n            if (async && callback) callback(getCompressedDataFromZip());\n            return getCompressedDataFromZip();\n        }\n\n        if (uncompressedData.length && !_isDirectory) {\n            var compressedData;\n            // Local file header\n            switch (_centralHeader.method) {\n                case Utils.Constants.STORED:\n                    _centralHeader.compressedSize = _centralHeader.size;\n\n                    compressedData = Buffer.alloc(uncompressedData.length);\n                    uncompressedData.copy(compressedData);\n\n                    if (async && callback) callback(compressedData);\n                    return compressedData;\n                default:\n                case Utils.Constants.DEFLATED:\n                    var deflater = new Methods.Deflater(uncompressedData);\n                    if (!async) {\n                        var deflated = deflater.deflate();\n                        _centralHeader.compressedSize = deflated.length;\n                        return deflated;\n                    } else {\n                        deflater.deflateAsync(function (data) {\n                            compressedData = Buffer.alloc(data.length);\n                            _centralHeader.compressedSize = data.length;\n                            data.copy(compressedData);\n                            callback && callback(compressedData);\n                        });\n                    }\n                    deflater = null;\n                    break;\n            }\n        } else if (async && callback) {\n            callback(Buffer.alloc(0));\n        } else {\n            return Buffer.alloc(0);\n        }\n    }\n\n    function readUInt64LE(buffer, offset) {\n        return (buffer.readUInt32LE(offset + 4) << 4) + buffer.readUInt32LE(offset);\n    }\n\n    function parseExtra(data) {\n        try {\n            var offset = 0;\n            var signature, size, part;\n            while (offset + 4 < data.length) {\n                signature = data.readUInt16LE(offset);\n                offset += 2;\n                size = data.readUInt16LE(offset);\n                offset += 2;\n                part = data.slice(offset, offset + size);\n                offset += size;\n                if (Constants.ID_ZIP64 === signature) {\n                    parseZip64ExtendedInformation(part);\n                }\n            }\n        } catch (error) {\n            throw Utils.Errors.EXTRA_FIELD_PARSE_ERROR();\n        }\n    }\n\n    //Override header field values with values from the ZIP64 extra field\n    function parseZip64ExtendedInformation(data) {\n        var size, compressedSize, offset, diskNumStart;\n\n        if (data.length >= Constants.EF_ZIP64_SCOMP) {\n            size = readUInt64LE(data, Constants.EF_ZIP64_SUNCOMP);\n            if (_centralHeader.size === Constants.EF_ZIP64_OR_32) {\n                _centralHeader.size = size;\n            }\n        }\n        if (data.length >= Constants.EF_ZIP64_RHO) {\n            compressedSize = readUInt64LE(data, Constants.EF_ZIP64_SCOMP);\n            if (_centralHeader.compressedSize === Constants.EF_ZIP64_OR_32) {\n                _centralHeader.compressedSize = compressedSize;\n            }\n        }\n        if (data.length >= Constants.EF_ZIP64_DSN) {\n            offset = readUInt64LE(data, Constants.EF_ZIP64_RHO);\n            if (_centralHeader.offset === Constants.EF_ZIP64_OR_32) {\n                _centralHeader.offset = offset;\n            }\n        }\n        if (data.length >= Constants.EF_ZIP64_DSN + 4) {\n            diskNumStart = data.readUInt32LE(Constants.EF_ZIP64_DSN);\n            if (_centralHeader.diskNumStart === Constants.EF_ZIP64_OR_16) {\n                _centralHeader.diskNumStart = diskNumStart;\n            }\n        }\n    }\n\n    return {\n        get entryName() {\n            return decoder.decode(_entryName);\n        },\n        get rawEntryName() {\n            return _entryName;\n        },\n        set entryName(val) {\n            _entryName = Utils.toBuffer(val, decoder.encode);\n            var lastChar = _entryName[_entryName.length - 1];\n            _isDirectory = lastChar === 47 || lastChar === 92;\n            _centralHeader.fileNameLength = _entryName.length;\n        },\n\n        get efs() {\n            if (typeof _efs === \"function\") {\n                return _efs(this.entryName);\n            } else {\n                return _efs;\n            }\n        },\n\n        get extra() {\n            return _extra;\n        },\n        set extra(val) {\n            _extra = val;\n            _centralHeader.extraLength = val.length;\n            parseExtra(val);\n        },\n\n        get comment() {\n            return decoder.decode(_comment);\n        },\n        set comment(val) {\n            _comment = Utils.toBuffer(val, decoder.encode);\n            _centralHeader.commentLength = _comment.length;\n            if (_comment.length > 0xffff) throw Utils.Errors.COMMENT_TOO_LONG();\n        },\n\n        get name() {\n            var n = decoder.decode(_entryName);\n            return _isDirectory\n                ? n\n                      .substr(n.length - 1)\n                      .split(\"/\")\n                      .pop()\n                : n.split(\"/\").pop();\n        },\n        get isDirectory() {\n            return _isDirectory;\n        },\n\n        getCompressedData: function () {\n            return compress(false, null);\n        },\n\n        getCompressedDataAsync: function (/*Function*/ callback) {\n            compress(true, callback);\n        },\n\n        setData: function (value) {\n            uncompressedData = Utils.toBuffer(value, Utils.decoder.encode);\n            if (!_isDirectory && uncompressedData.length) {\n                _centralHeader.size = uncompressedData.length;\n                _centralHeader.method = Utils.Constants.DEFLATED;\n                _centralHeader.crc = Utils.crc32(value);\n                _centralHeader.changed = true;\n            } else {\n                // folders and blank files should be stored\n                _centralHeader.method = Utils.Constants.STORED;\n            }\n        },\n\n        getData: function (pass) {\n            if (_centralHeader.changed) {\n                return uncompressedData;\n            } else {\n                return decompress(false, null, pass);\n            }\n        },\n\n        getDataAsync: function (/*Function*/ callback, pass) {\n            if (_centralHeader.changed) {\n                callback(uncompressedData);\n            } else {\n                decompress(true, callback, pass);\n            }\n        },\n\n        set attr(attr) {\n            _centralHeader.attr = attr;\n        },\n        get attr() {\n            return _centralHeader.attr;\n        },\n\n        set header(/*Buffer*/ data) {\n            _centralHeader.loadFromBinary(data);\n        },\n\n        get header() {\n            return _centralHeader;\n        },\n\n        packCentralHeader: function () {\n            _centralHeader.flags_efs = this.efs;\n            _centralHeader.extraLength = _extra.length;\n            // 1. create header (buffer)\n            var header = _centralHeader.centralHeaderToBinary();\n            var addpos = Utils.Constants.CENHDR;\n            // 2. add file name\n            _entryName.copy(header, addpos);\n            addpos += _entryName.length;\n            // 3. add extra data\n            _extra.copy(header, addpos);\n            addpos += _centralHeader.extraLength;\n            // 4. add file comment\n            _comment.copy(header, addpos);\n            return header;\n        },\n\n        packLocalHeader: function () {\n            let addpos = 0;\n            _centralHeader.flags_efs = this.efs;\n            _centralHeader.extraLocalLength = _extralocal.length;\n            // 1. construct local header Buffer\n            const localHeaderBuf = _centralHeader.localHeaderToBinary();\n            // 2. localHeader - crate header buffer\n            const localHeader = Buffer.alloc(localHeaderBuf.length + _entryName.length + _centralHeader.extraLocalLength);\n            // 2.1 add localheader\n            localHeaderBuf.copy(localHeader, addpos);\n            addpos += localHeaderBuf.length;\n            // 2.2 add file name\n            _entryName.copy(localHeader, addpos);\n            addpos += _entryName.length;\n            // 2.3 add extra field\n            _extralocal.copy(localHeader, addpos);\n            addpos += _extralocal.length;\n\n            return localHeader;\n        },\n\n        toJSON: function () {\n            const bytes = function (nr) {\n                return \"<\" + ((nr && nr.length + \" bytes buffer\") || \"null\") + \">\";\n            };\n\n            return {\n                entryName: this.entryName,\n                name: this.name,\n                comment: this.comment,\n                isDirectory: this.isDirectory,\n                header: _centralHeader.toJSON(),\n                compressedData: bytes(input),\n                data: bytes(uncompressedData)\n            };\n        },\n\n        toString: function () {\n            return JSON.stringify(this.toJSON(), null, \"\\t\");\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/zipEntry.js?");

/***/ }),

/***/ "../../node_modules/adm-zip/zipFile.js":
/*!*********************************************!*\
  !*** ../../node_modules/adm-zip/zipFile.js ***!
  \*********************************************/
/***/ ((module, __unused_webpack_exports, __webpack_require__) => {

eval("const ZipEntry = __webpack_require__(/*! ./zipEntry */ \"../../node_modules/adm-zip/zipEntry.js\");\nconst Headers = __webpack_require__(/*! ./headers */ \"../../node_modules/adm-zip/headers/index.js\");\nconst Utils = __webpack_require__(/*! ./util */ \"../../node_modules/adm-zip/util/index.js\");\n\nmodule.exports = function (/*Buffer|null*/ inBuffer, /** object */ options) {\n    var entryList = [],\n        entryTable = {},\n        _comment = Buffer.alloc(0),\n        mainHeader = new Headers.MainHeader(),\n        loadedEntries = false;\n    var password = null;\n    const temporary = new Set();\n\n    // assign options\n    const opts = options;\n\n    const { noSort, decoder } = opts;\n\n    if (inBuffer) {\n        // is a memory buffer\n        readMainHeader(opts.readEntries);\n    } else {\n        // none. is a new file\n        loadedEntries = true;\n    }\n\n    function makeTemporaryFolders() {\n        const foldersList = new Set();\n\n        // Make list of all folders in file\n        for (const elem of Object.keys(entryTable)) {\n            const elements = elem.split(\"/\");\n            elements.pop(); // filename\n            if (!elements.length) continue; // no folders\n            for (let i = 0; i < elements.length; i++) {\n                const sub = elements.slice(0, i + 1).join(\"/\") + \"/\";\n                foldersList.add(sub);\n            }\n        }\n\n        // create missing folders as temporary\n        for (const elem of foldersList) {\n            if (!(elem in entryTable)) {\n                const tempfolder = new ZipEntry(opts);\n                tempfolder.entryName = elem;\n                tempfolder.attr = 0x10;\n                tempfolder.temporary = true;\n                entryList.push(tempfolder);\n                entryTable[tempfolder.entryName] = tempfolder;\n                temporary.add(tempfolder);\n            }\n        }\n    }\n\n    function readEntries() {\n        loadedEntries = true;\n        entryTable = {};\n        if (mainHeader.diskEntries > (inBuffer.length - mainHeader.offset) / Utils.Constants.CENHDR) {\n            throw Utils.Errors.DISK_ENTRY_TOO_LARGE();\n        }\n        entryList = new Array(mainHeader.diskEntries); // total number of entries\n        var index = mainHeader.offset; // offset of first CEN header\n        for (var i = 0; i < entryList.length; i++) {\n            var tmp = index,\n                entry = new ZipEntry(opts, inBuffer);\n            entry.header = inBuffer.slice(tmp, (tmp += Utils.Constants.CENHDR));\n\n            entry.entryName = inBuffer.slice(tmp, (tmp += entry.header.fileNameLength));\n\n            if (entry.header.extraLength) {\n                entry.extra = inBuffer.slice(tmp, (tmp += entry.header.extraLength));\n            }\n\n            if (entry.header.commentLength) entry.comment = inBuffer.slice(tmp, tmp + entry.header.commentLength);\n\n            index += entry.header.centralHeaderSize;\n\n            entryList[i] = entry;\n            entryTable[entry.entryName] = entry;\n        }\n        temporary.clear();\n        makeTemporaryFolders();\n    }\n\n    function readMainHeader(/*Boolean*/ readNow) {\n        var i = inBuffer.length - Utils.Constants.ENDHDR, // END header size\n            max = Math.max(0, i - 0xffff), // 0xFFFF is the max zip file comment length\n            n = max,\n            endStart = inBuffer.length,\n            endOffset = -1, // Start offset of the END header\n            commentEnd = 0;\n\n        // option to search header form entire file\n        const trailingSpace = typeof opts.trailingSpace === \"boolean\" ? opts.trailingSpace : false;\n        if (trailingSpace) max = 0;\n\n        for (i; i >= n; i--) {\n            if (inBuffer[i] !== 0x50) continue; // quick check that the byte is 'P'\n            if (inBuffer.readUInt32LE(i) === Utils.Constants.ENDSIG) {\n                // \"PK\\005\\006\"\n                endOffset = i;\n                commentEnd = i;\n                endStart = i + Utils.Constants.ENDHDR;\n                // We already found a regular signature, let's look just a bit further to check if there's any zip64 signature\n                n = i - Utils.Constants.END64HDR;\n                continue;\n            }\n\n            if (inBuffer.readUInt32LE(i) === Utils.Constants.END64SIG) {\n                // Found a zip64 signature, let's continue reading the whole zip64 record\n                n = max;\n                continue;\n            }\n\n            if (inBuffer.readUInt32LE(i) === Utils.Constants.ZIP64SIG) {\n                // Found the zip64 record, let's determine it's size\n                endOffset = i;\n                endStart = i + Utils.readBigUInt64LE(inBuffer, i + Utils.Constants.ZIP64SIZE) + Utils.Constants.ZIP64LEAD;\n                break;\n            }\n        }\n\n        if (endOffset == -1) throw Utils.Errors.INVALID_FORMAT();\n\n        mainHeader.loadFromBinary(inBuffer.slice(endOffset, endStart));\n        if (mainHeader.commentLength) {\n            _comment = inBuffer.slice(commentEnd + Utils.Constants.ENDHDR);\n        }\n        if (readNow) readEntries();\n    }\n\n    function sortEntries() {\n        if (entryList.length > 1 && !noSort) {\n            entryList.sort((a, b) => a.entryName.toLowerCase().localeCompare(b.entryName.toLowerCase()));\n        }\n    }\n\n    return {\n        /**\n         * Returns an array of ZipEntry objects existent in the current opened archive\n         * @return Array\n         */\n        get entries() {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            return entryList.filter((e) => !temporary.has(e));\n        },\n\n        /**\n         * Archive comment\n         * @return {String}\n         */\n        get comment() {\n            return decoder.decode(_comment);\n        },\n        set comment(val) {\n            _comment = Utils.toBuffer(val, decoder.encode);\n            mainHeader.commentLength = _comment.length;\n        },\n\n        getEntryCount: function () {\n            if (!loadedEntries) {\n                return mainHeader.diskEntries;\n            }\n\n            return entryList.length;\n        },\n\n        forEach: function (callback) {\n            this.entries.forEach(callback);\n        },\n\n        /**\n         * Returns a reference to the entry with the given name or null if entry is inexistent\n         *\n         * @param entryName\n         * @return ZipEntry\n         */\n        getEntry: function (/*String*/ entryName) {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            return entryTable[entryName] || null;\n        },\n\n        /**\n         * Adds the given entry to the entry list\n         *\n         * @param entry\n         */\n        setEntry: function (/*ZipEntry*/ entry) {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            entryList.push(entry);\n            entryTable[entry.entryName] = entry;\n            mainHeader.totalEntries = entryList.length;\n        },\n\n        /**\n         * Removes the file with the given name from the entry list.\n         *\n         * If the entry is a directory, then all nested files and directories will be removed\n         * @param entryName\n         * @returns {void}\n         */\n        deleteFile: function (/*String*/ entryName, withsubfolders = true) {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            const entry = entryTable[entryName];\n            const list = this.getEntryChildren(entry, withsubfolders).map((child) => child.entryName);\n\n            list.forEach(this.deleteEntry);\n        },\n\n        /**\n         * Removes the entry with the given name from the entry list.\n         *\n         * @param {string} entryName\n         * @returns {void}\n         */\n        deleteEntry: function (/*String*/ entryName) {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            const entry = entryTable[entryName];\n            const index = entryList.indexOf(entry);\n            if (index >= 0) {\n                entryList.splice(index, 1);\n                delete entryTable[entryName];\n                mainHeader.totalEntries = entryList.length;\n            }\n        },\n\n        /**\n         *  Iterates and returns all nested files and directories of the given entry\n         *\n         * @param entry\n         * @return Array\n         */\n        getEntryChildren: function (/*ZipEntry*/ entry, subfolders = true) {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            if (typeof entry === \"object\") {\n                if (entry.isDirectory && subfolders) {\n                    const list = [];\n                    const name = entry.entryName;\n\n                    for (const zipEntry of entryList) {\n                        if (zipEntry.entryName.startsWith(name)) {\n                            list.push(zipEntry);\n                        }\n                    }\n                    return list;\n                } else {\n                    return [entry];\n                }\n            }\n            return [];\n        },\n\n        /**\n         *  How many child elements entry has\n         *\n         * @param {ZipEntry} entry\n         * @return {integer}\n         */\n        getChildCount: function (entry) {\n            if (entry && entry.isDirectory) {\n                const list = this.getEntryChildren(entry);\n                return list.includes(entry) ? list.length - 1 : list.length;\n            }\n            return 0;\n        },\n\n        /**\n         * Returns the zip file\n         *\n         * @return Buffer\n         */\n        compressToBuffer: function () {\n            if (!loadedEntries) {\n                readEntries();\n            }\n            sortEntries();\n\n            const dataBlock = [];\n            const headerBlocks = [];\n            let totalSize = 0;\n            let dindex = 0;\n\n            mainHeader.size = 0;\n            mainHeader.offset = 0;\n            let totalEntries = 0;\n\n            for (const entry of this.entries) {\n                // compress data and set local and entry header accordingly. Reason why is called first\n                const compressedData = entry.getCompressedData();\n                entry.header.offset = dindex;\n\n                // 1. construct local header\n                const localHeader = entry.packLocalHeader();\n\n                // 2. offsets\n                const dataLength = localHeader.length + compressedData.length;\n                dindex += dataLength;\n\n                // 3. store values in sequence\n                dataBlock.push(localHeader);\n                dataBlock.push(compressedData);\n\n                // 4. construct central header\n                const centralHeader = entry.packCentralHeader();\n                headerBlocks.push(centralHeader);\n                // 5. update main header\n                mainHeader.size += centralHeader.length;\n                totalSize += dataLength + centralHeader.length;\n                totalEntries++;\n            }\n\n            totalSize += mainHeader.mainHeaderSize; // also includes zip file comment length\n            // point to end of data and beginning of central directory first record\n            mainHeader.offset = dindex;\n            mainHeader.totalEntries = totalEntries;\n\n            dindex = 0;\n            const outBuffer = Buffer.alloc(totalSize);\n            // write data blocks\n            for (const content of dataBlock) {\n                content.copy(outBuffer, dindex);\n                dindex += content.length;\n            }\n\n            // write central directory entries\n            for (const content of headerBlocks) {\n                content.copy(outBuffer, dindex);\n                dindex += content.length;\n            }\n\n            // write main header\n            const mh = mainHeader.toBinary();\n            if (_comment) {\n                _comment.copy(mh, Utils.Constants.ENDHDR); // add zip file comment\n            }\n            mh.copy(outBuffer, dindex);\n\n            // Since we update entry and main header offsets,\n            // they are no longer valid and we have to reset content\n            // (Issue 64)\n\n            inBuffer = outBuffer;\n            loadedEntries = false;\n\n            return outBuffer;\n        },\n\n        toAsyncBuffer: function (/*Function*/ onSuccess, /*Function*/ onFail, /*Function*/ onItemStart, /*Function*/ onItemEnd) {\n            try {\n                if (!loadedEntries) {\n                    readEntries();\n                }\n                sortEntries();\n\n                const dataBlock = [];\n                const centralHeaders = [];\n                let totalSize = 0;\n                let dindex = 0;\n                let totalEntries = 0;\n\n                mainHeader.size = 0;\n                mainHeader.offset = 0;\n\n                const compress2Buffer = function (entryLists) {\n                    if (entryLists.length > 0) {\n                        const entry = entryLists.shift();\n                        const name = entry.entryName + entry.extra.toString();\n                        if (onItemStart) onItemStart(name);\n                        entry.getCompressedDataAsync(function (compressedData) {\n                            if (onItemEnd) onItemEnd(name);\n                            entry.header.offset = dindex;\n\n                            // 1. construct local header\n                            const localHeader = entry.packLocalHeader();\n\n                            // 2. offsets\n                            const dataLength = localHeader.length + compressedData.length;\n                            dindex += dataLength;\n\n                            // 3. store values in sequence\n                            dataBlock.push(localHeader);\n                            dataBlock.push(compressedData);\n\n                            // central header\n                            const centalHeader = entry.packCentralHeader();\n                            centralHeaders.push(centalHeader);\n                            mainHeader.size += centalHeader.length;\n                            totalSize += dataLength + centalHeader.length;\n                            totalEntries++;\n\n                            compress2Buffer(entryLists);\n                        });\n                    } else {\n                        totalSize += mainHeader.mainHeaderSize; // also includes zip file comment length\n                        // point to end of data and beginning of central directory first record\n                        mainHeader.offset = dindex;\n                        mainHeader.totalEntries = totalEntries;\n\n                        dindex = 0;\n                        const outBuffer = Buffer.alloc(totalSize);\n                        dataBlock.forEach(function (content) {\n                            content.copy(outBuffer, dindex); // write data blocks\n                            dindex += content.length;\n                        });\n                        centralHeaders.forEach(function (content) {\n                            content.copy(outBuffer, dindex); // write central directory entries\n                            dindex += content.length;\n                        });\n\n                        const mh = mainHeader.toBinary();\n                        if (_comment) {\n                            _comment.copy(mh, Utils.Constants.ENDHDR); // add zip file comment\n                        }\n\n                        mh.copy(outBuffer, dindex); // write main header\n\n                        // Since we update entry and main header offsets, they are no\n                        // longer valid and we have to reset content using our new buffer\n                        // (Issue 64)\n\n                        inBuffer = outBuffer;\n                        loadedEntries = false;\n\n                        onSuccess(outBuffer);\n                    }\n                };\n\n                compress2Buffer(Array.from(this.entries));\n            } catch (e) {\n                onFail(e);\n            }\n        }\n    };\n};\n\n\n//# sourceURL=webpack://lambda-api-handler/../../node_modules/adm-zip/zipFile.js?");

/***/ }),

/***/ "aws-sdk":
/*!**************************!*\
  !*** external "aws-sdk" ***!
  \**************************/
/***/ ((module) => {

"use strict";
module.exports = require("aws-sdk");

/***/ }),

/***/ "bcrypt":
/*!*************************!*\
  !*** external "bcrypt" ***!
  \*************************/
/***/ ((module) => {

"use strict";
module.exports = require("bcrypt");

/***/ }),

/***/ "dotenv":
/*!*************************!*\
  !*** external "dotenv" ***!
  \*************************/
/***/ ((module) => {

"use strict";
module.exports = require("dotenv");

/***/ }),

/***/ "jsonwebtoken":
/*!*******************************!*\
  !*** external "jsonwebtoken" ***!
  \*******************************/
/***/ ((module) => {

"use strict";
module.exports = require("jsonwebtoken");

/***/ }),

/***/ "node-fetch":
/*!*****************************!*\
  !*** external "node-fetch" ***!
  \*****************************/
/***/ ((module) => {

"use strict";
module.exports = require("node-fetch");

/***/ }),

/***/ "pg":
/*!*********************!*\
  !*** external "pg" ***!
  \*********************/
/***/ ((module) => {

"use strict";
module.exports = require("pg");

/***/ }),

/***/ "crypto":
/*!*************************!*\
  !*** external "crypto" ***!
  \*************************/
/***/ ((module) => {

"use strict";
module.exports = require("crypto");

/***/ }),

/***/ "fs":
/*!*********************!*\
  !*** external "fs" ***!
  \*********************/
/***/ ((module) => {

"use strict";
module.exports = require("fs");

/***/ }),

/***/ "path":
/*!***********************!*\
  !*** external "path" ***!
  \***********************/
/***/ ((module) => {

"use strict";
module.exports = require("path");

/***/ }),

/***/ "zlib":
/*!***********************!*\
  !*** external "zlib" ***!
  \***********************/
/***/ ((module) => {

"use strict";
module.exports = require("zlib");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/create fake namespace object */
/******/ 	(() => {
/******/ 		var getProto = Object.getPrototypeOf ? (obj) => (Object.getPrototypeOf(obj)) : (obj) => (obj.__proto__);
/******/ 		var leafPrototypes;
/******/ 		// create a fake namespace object
/******/ 		// mode & 1: value is a module id, require it
/******/ 		// mode & 2: merge all properties of value into the ns
/******/ 		// mode & 4: return value when already ns object
/******/ 		// mode & 16: return value when it's Promise-like
/******/ 		// mode & 8|1: behave like require
/******/ 		__webpack_require__.t = function(value, mode) {
/******/ 			if(mode & 1) value = this(value);
/******/ 			if(mode & 8) return value;
/******/ 			if(typeof value === 'object' && value) {
/******/ 				if((mode & 4) && value.__esModule) return value;
/******/ 				if((mode & 16) && typeof value.then === 'function') return value;
/******/ 			}
/******/ 			var ns = Object.create(null);
/******/ 			__webpack_require__.r(ns);
/******/ 			var def = {};
/******/ 			leafPrototypes = leafPrototypes || [null, getProto({}), getProto([]), getProto(getProto)];
/******/ 			for(var current = mode & 2 && value; typeof current == 'object' && !~leafPrototypes.indexOf(current); current = getProto(current)) {
/******/ 				Object.getOwnPropertyNames(current).forEach((key) => (def[key] = () => (value[key])));
/******/ 			}
/******/ 			def['default'] = () => (value);
/******/ 			__webpack_require__.d(ns, def);
/******/ 			return ns;
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./index.ts");
/******/ 	module.exports = __webpack_exports__;
/******/ 	
/******/ })()
;
>>>>>>> f637ce9 (fixed rating errors)
